{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f36f326048cdf5",
   "metadata": {},
   "source": [
    "### Assignment 1.2: Stats and visualization of the Rock Music Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af50f62",
   "metadata": {},
   "source": [
    "#### *Step 1: Get a clean list of page titles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5274079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Parsing titles...\n",
      "Saved 489 titles to rock_artists.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "PDF_PATH = \"rock_artists.pdf\"\n",
    "TXT_EXPORT = \"rock_artists_raw.txt\"\n",
    "OUT_LIST = \"rock_artists.txt\"\n",
    "\n",
    "def pdf_to_text(pdf_path, txt_path):\n",
    "    import PyPDF2\n",
    "    with open(pdf_path, \"rb\") as f, open(txt_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            out.write(page.extract_text() or \"\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "def parse_titles(text):\n",
    "    titles = set()\n",
    "\n",
    "    # Preferred: wiki-style [[Title]] or [[Title|display]]\n",
    "    for raw in re.findall(r\"\\[\\[([^\\[\\]]+)\\]\\]\", text):\n",
    "        left = raw.split(\"|\", 1)[0].strip()\n",
    "        if not left or \":\" in left:  # skip namespaces like Category:\n",
    "            continue\n",
    "        titles.add(left)\n",
    "\n",
    "    if not titles:\n",
    "        # Fallback: lines that look like Proper Noun titles (very heuristic)\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if 2 <= len(line) <= 120 and line[0].isupper() and not line.endswith(\":\"):\n",
    "                # crude filter to avoid headers; adjust as needed\n",
    "                titles.add(line)\n",
    "\n",
    "    # Normalize: keep canonical version with spaces; downloader will add underscores\n",
    "    clean = []\n",
    "    seen = set()\n",
    "    for t in titles:\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "        if t and t not in seen:\n",
    "            seen.add(t)\n",
    "            clean.append(t)\n",
    "    clean.sort()\n",
    "    return clean\n",
    "\n",
    "def main():\n",
    "    if not Path(PDF_PATH).exists():\n",
    "        raise SystemExit(f\"PDF not found: {PDF_PATH}. Put your PDF next to this script.\")\n",
    "\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    pdf_to_text(PDF_PATH, TXT_EXPORT)\n",
    "    text = Path(TXT_EXPORT).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    print(\"Parsing titles...\")\n",
    "    titles = parse_titles(text)\n",
    "    if not titles:\n",
    "        raise SystemExit(\"Found 0 titles. Open rock_artists_raw.txt and check the formatting.\")\n",
    "\n",
    "    Path(OUT_LIST).write_text(\"\\n\".join(titles), encoding=\"utf-8\")\n",
    "    print(f\"Saved {len(titles)} titles to {OUT_LIST}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fab465",
   "metadata": {},
   "source": [
    "#### *Step 2: Fetch raw Wikipedia wikitext for each title*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a71a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 10_Years_(band).txt (28468 bytes)\n",
      "Saved: 10cc.txt (67435 bytes)\n",
      "Saved: 3_Doors_Down.txt (46796 bytes)\n",
      "Saved: 311_(band).txt (55878 bytes)\n",
      "Saved: 38_Special_(band).txt (29277 bytes)\n",
      "Saved: A_Perfect_Circle.txt (89175 bytes)\n",
      "Saved: ABBA.txt (152243 bytes)\n",
      "Saved: AC_DC.txt (183971 bytes)\n",
      "Saved: AFI_(band).txt (61606 bytes)\n",
      "Saved: Accept_(band).txt (47788 bytes)\n",
      "Saved: Adam_Ant.txt (94230 bytes)\n",
      "Saved: Aerosmith.txt (192596 bytes)\n",
      "Saved: Air_Supply.txt (52244 bytes)\n",
      "Saved: Alanis_Morissette.txt (122316 bytes)\n",
      "Saved: Alice_Cooper.txt (181017 bytes)\n",
      "Saved: Alice_Cooper_(band).txt (39483 bytes)\n",
      "Saved: Alice_in_Chains.txt (204602 bytes)\n",
      "Saved: AllMusic.txt (13015 bytes)\n",
      "Saved: Alter_Bridge.txt (78514 bytes)\n",
      "Saved: Ambrosia_(band).txt (30410 bytes)\n",
      "Saved: America_(band).txt (52762 bytes)\n",
      "Saved: Anthrax_(American_band).txt (91898 bytes)\n",
      "Saved: April_Wine.txt (37328 bytes)\n",
      "Saved: Arcade_Fire.txt (106126 bytes)\n",
      "Saved: Arctic_Monkeys.txt (160138 bytes)\n",
      "Saved: Asia_(band).txt (59660 bytes)\n",
      "Saved: Audioslave.txt (87720 bytes)\n",
      "Saved: Avenged_Sevenfold.txt (106162 bytes)\n",
      "Saved: Avril_Lavigne.txt (247390 bytes)\n",
      "Saved: Awolnation.txt (59979 bytes)\n",
      "Saved: Bachman_–Turner_Overdrive.txt (0 bytes)\n",
      "Saved: Bad_Company.txt (34775 bytes)\n",
      "Saved: Badfinger.txt (93637 bytes)\n",
      "Saved: Barenaked_Ladies.txt (82721 bytes)\n",
      "Saved: Bay_City_Rollers.txt (79812 bytes)\n",
      "Saved: Beastie_Boys.txt (130066 bytes)\n",
      "Saved: Beck.txt (122512 bytes)\n",
      "Saved: Bee_Gees.txt (143063 bytes)\n",
      "Saved: Ben_Folds_Five.txt (25611 bytes)\n",
      "Saved: Bill_Haley_&_His_Comets.txt (62161 bytes)\n",
      "Saved: Billy_Idol.txt (65979 bytes)\n",
      "Saved: Billy_Joel.txt (180906 bytes)\n",
      "Saved: Billy_Squier.txt (28033 bytes)\n",
      "Saved: Billy_Talent.txt (49030 bytes)\n",
      "Saved: Black_Sabbath.txt (188806 bytes)\n",
      "Saved: Black_Stone_Cherry.txt (19796 bytes)\n",
      "Saved: Black_Veil_Brides.txt (96105 bytes)\n",
      "Saved: Blink-182.txt (141436 bytes)\n",
      "Saved: Blondie_(band).txt (76658 bytes)\n",
      "Saved: Bloodhound_Gang.txt (32867 bytes)\n",
      "Saved: Blue_October.txt (39081 bytes)\n",
      "Saved: Blue_Öyster_Cult.txt (68229 bytes)\n",
      "Saved: Blues_Traveler.txt (33127 bytes)\n",
      "Saved: Blur_(band).txt (81406 bytes)\n",
      "Saved: Bo_Diddley.txt (81429 bytes)\n",
      "Saved: Bob_Dylan.txt (264174 bytes)\n",
      "Saved: Bob_Seger.txt (50393 bytes)\n",
      "Saved: Bon_Jovi.txt (89616 bytes)\n",
      "Saved: Boston_(band).txt (52682 bytes)\n",
      "Saved: Bowling_for_Soup.txt (66003 bytes)\n",
      "Saved: Boys_Like_Girls.txt (61385 bytes)\n",
      "Saved: Bread_(band).txt (38282 bytes)\n",
      "Saved: Breaking_Benjamin.txt (109080 bytes)\n",
      "Saved: Bring_Me_the_Horizon.txt (197042 bytes)\n",
      "Saved: Bruce_Springsteen.txt (216261 bytes)\n",
      "Saved: Bryan_Adams.txt (248100 bytes)\n",
      "Saved: Bryan_Ferry.txt (73735 bytes)\n",
      "Saved: Buckcherry.txt (31717 bytes)\n",
      "Saved: Buddy_Holly.txt (79280 bytes)\n",
      "Saved: Bullet_for_My_Valentine.txt (123481 bytes)\n",
      "Saved: Bush_(British_band).txt (46333 bytes)\n",
      "Saved: Cage_the_Elephant.txt (48281 bytes)\n",
      "Saved: Cake_(band).txt (58884 bytes)\n",
      "Saved: Canned_Heat.txt (39966 bytes)\n",
      "Saved: Catfish_and_the_Bottlemen.txt (62179 bytes)\n",
      "Saved: Charlie_Daniels.txt (50145 bytes)\n",
      "Saved: Cheap_Trick.txt (44302 bytes)\n",
      "Saved: Chevelle_(band).txt (102084 bytes)\n",
      "Saved: Chicago_(band).txt (244762 bytes)\n",
      "Saved: Chris_Cornell.txt (218933 bytes)\n",
      "Saved: Chris_Rea.txt (68030 bytes)\n",
      "Saved: Chris_de_Burgh.txt (27296 bytes)\n",
      "Saved: Christopher_Cross.txt (43402 bytes)\n",
      "Saved: Chubby_Checker.txt (46174 bytes)\n",
      "Saved: Chuck_Berry.txt (79957 bytes)\n",
      "Saved: Cinderella_(band).txt (26247 bytes)\n",
      "Saved: Coheed_and_Cambria.txt (73030 bytes)\n",
      "Saved: Cold_Chisel.txt (74591 bytes)\n",
      "Saved: Coldplay.txt (294838 bytes)\n",
      "Saved: Collective_Soul.txt (37231 bytes)\n",
      "Saved: Counting_Crows.txt (59764 bytes)\n",
      "Saved: Crash_Test_Dummies.txt (34393 bytes)\n",
      "Saved: Cream_(band).txt (41072 bytes)\n",
      "Saved: Creed_(band).txt (78856 bytes)\n",
      "Saved: Creedence_Clearwater_Revival.txt (65424 bytes)\n",
      "Saved: Crosby,_Stills,_Nash_&_Young.txt (96975 bytes)\n",
      "Saved: Crowded_House.txt (67157 bytes)\n",
      "Saved: Buddy_Holly.txt (79280 bytes)\n",
      "Saved: Damn_Yankees_(band).txt (16329 bytes)\n",
      "Saved: Dan_Fogelberg.txt (24019 bytes)\n",
      "Saved: Dashboard_Confessional.txt (39190 bytes)\n",
      "Saved: Daughtry_(band).txt (52389 bytes)\n",
      "Saved: Dave_Matthews_Band.txt (79173 bytes)\n",
      "Saved: David_Bowie.txt (261282 bytes)\n",
      "Saved: Days_of_the_New.txt (37336 bytes)\n",
      "Saved: Death_Cab_for_Cutie.txt (63964 bytes)\n",
      "Saved: Deep_Purple.txt (142082 bytes)\n",
      "Saved: Def_Leppard.txt (102123 bytes)\n",
      "Saved: Deftones.txt (133350 bytes)\n",
      "Saved: Depeche_Mode.txt (133494 bytes)\n",
      "Saved: Dio_(band).txt (31954 bytes)\n",
      "Saved: Dire_Straits.txt (79190 bytes)\n",
      "Saved: Disturbed_(band).txt (75607 bytes)\n",
      "Saved: Don_Henley.txt (69044 bytes)\n",
      "Saved: Donovan.txt (74618 bytes)\n",
      "Saved: Depeche_Mode.txt (133494 bytes)\n",
      "Saved: Dropkick_Murphys.txt (88816 bytes)\n",
      "Saved: Drowning_Pool.txt (36692 bytes)\n",
      "Saved: Duane_Eddy.txt (64475 bytes)\n",
      "Saved: Duran_Duran.txt (122439 bytes)\n",
      "Saved: Eagles_(band).txt (102072 bytes)\n",
      "Saved: Echo_&_the_Bunnymen.txt (47526 bytes)\n",
      "Saved: Eddie_Cochran.txt (47080 bytes)\n",
      "Saved: Eddie_Money.txt (37081 bytes)\n",
      "Saved: Edgar_Winter.txt (40690 bytes)\n",
      "Saved: Electric_Light_Orchestra.txt (59667 bytes)\n",
      "Saved: Elton_John.txt (256840 bytes)\n",
      "Saved: Elvis_Costello.txt (179983 bytes)\n",
      "Saved: Elvis_Presley.txt (215098 bytes)\n",
      "Saved: Emerson,_Lake_&_Palmer.txt (66066 bytes)\n",
      "Saved: England_Dan_&_John_Ford_Coley.txt (15371 bytes)\n",
      "Saved: Eric_Clapton.txt (200541 bytes)\n",
      "Saved: Europe_(band).txt (51642 bytes)\n",
      "Saved: Evanescence.txt (184256 bytes)\n",
      "Saved: Everclear_(band).txt (37454 bytes)\n",
      "Saved: Everlast.txt (26856 bytes)\n",
      "Saved: Extreme_(band).txt (25151 bytes)\n",
      "Saved: Faces_(band).txt (37287 bytes)\n",
      "Saved: Faith_No_More.txt (96778 bytes)\n",
      "Saved: Fall_Out_Boy.txt (232732 bytes)\n",
      "Saved: Fats_Domino.txt (53471 bytes)\n",
      "Saved: Filter_(band).txt (51433 bytes)\n",
      "Saved: Finger_Eleven.txt (38270 bytes)\n",
      "Saved: FireHouse.txt (22639 bytes)\n",
      "Saved: Five_Finger_Death_Punch.txt (104751 bytes)\n",
      "Saved: Five_for_Fighting.txt (72381 bytes)\n",
      "Saved: Fleetwood_Mac.txt (125399 bytes)\n",
      "Saved: Flogging_Molly.txt (17601 bytes)\n",
      "Saved: Florence_and_the_Machine.txt (108869 bytes)\n",
      "Saved: Flyleaf_(band).txt (45694 bytes)\n",
      "Saved: Foals_(band).txt (43350 bytes)\n",
      "Saved: Foo_Fighters.txt (122516 bytes)\n",
      "Saved: Foreigner_(band).txt (76189 bytes)\n",
      "Saved: Foster_the_People.txt (67085 bytes)\n",
      "Saved: Frank_Zappa.txt (219180 bytes)\n",
      "Saved: Franz_Ferdinand_(band).txt (54186 bytes)\n",
      "Saved: Fuel_(band).txt (35297 bytes)\n",
      "Saved: Fun_(band).txt (25152 bytes)\n",
      "Saved: Funkadelic.txt (18848 bytes)\n",
      "Saved: Garbage_(band).txt (103859 bytes)\n",
      "Saved: Gary_Glitter.txt (88864 bytes)\n",
      "Saved: Gene_Vincent.txt (45505 bytes)\n",
      "Saved: Genesis_(band).txt (155546 bytes)\n",
      "Saved: George_Harrison.txt (167222 bytes)\n",
      "Saved: George_Thorogood.txt (41131 bytes)\n",
      "Saved: Ghost_(Swedish_band).txt (163629 bytes)\n",
      "Saved: Gin_Blossoms.txt (31080 bytes)\n",
      "Saved: Glenn_Frey.txt (56291 bytes)\n",
      "Saved: Godsmack.txt (90298 bytes)\n",
      "Saved: Golden_Earring.txt (25844 bytes)\n",
      "Saved: Goo_Goo_Dolls.txt (51233 bytes)\n",
      "Saved: Good_Charlotte.txt (80153 bytes)\n",
      "Saved: Grand_Funk_Railroad.txt (33294 bytes)\n",
      "Saved: Grateful_Dead.txt (134984 bytes)\n",
      "Saved: Great_White.txt (60854 bytes)\n",
      "Saved: Green_Day.txt (218381 bytes)\n",
      "Saved: Greta_Van_Fleet.txt (61276 bytes)\n",
      "Saved: Guns_N'_Roses.txt (370245 bytes)\n",
      "Saved: Halestorm.txt (39602 bytes)\n",
      "Saved: Hall_&_Oates.txt (75101 bytes)\n",
      "Saved: Harry_Chapin.txt (41704 bytes)\n",
      "Saved: Harry_Nilsson.txt (55372 bytes)\n",
      "Saved: Heart_(band).txt (83843 bytes)\n",
      "Saved: Herman's_Hermits.txt (39552 bytes)\n",
      "Saved: Highly_Suspect.txt (32116 bytes)\n",
      "Saved: Hinder.txt (40046 bytes)\n",
      "Saved: Hole_(band).txt (104116 bytes)\n",
      "Saved: Hoobastank.txt (30670 bytes)\n",
      "Saved: Hootie_&_the_Blowfish.txt (29465 bytes)\n",
      "Saved: Huey_Lewis_and_the_News.txt (35602 bytes)\n",
      "Saved: INXS.txt (91294 bytes)\n",
      "Saved: Ian_Dury.txt (54459 bytes)\n",
      "Saved: Icehouse_(band).txt (63956 bytes)\n",
      "Saved: Iggy_Pop.txt (108998 bytes)\n",
      "Saved: Imagine_Dragons.txt (112557 bytes)\n",
      "Saved: Incubus_(band).txt (69877 bytes)\n",
      "Saved: Interpol_(band).txt (40113 bytes)\n",
      "Saved: Iron_Maiden.txt (173893 bytes)\n",
      "Saved: Jack_White.txt (144184 bytes)\n",
      "Saved: Jackson_Browne.txt (54159 bytes)\n",
      "Saved: James_Blunt.txt (74903 bytes)\n",
      "Saved: James_Taylor.txt (90638 bytes)\n",
      "Saved: Jane's_Addiction.txt (90417 bytes)\n",
      "Saved: Janis_Joplin.txt (142710 bytes)\n",
      "Saved: Jeff_Buckley.txt (85433 bytes)\n",
      "Saved: Jefferson_Airplane.txt (83588 bytes)\n",
      "Saved: Jefferson_Starship.txt (66679 bytes)\n",
      "Saved: Jerry_Lee_Lewis.txt (94366 bytes)\n",
      "Saved: John_Lennon.txt (189482 bytes)\n",
      "Saved: Jethro_Tull_(band).txt (77465 bytes)\n",
      "Saved: Jim_Croce.txt (28845 bytes)\n",
      "Saved: Jimi_Hendrix.txt (179515 bytes)\n",
      "Saved: Jimmy_Eat_World.txt (50347 bytes)\n",
      "Saved: Joan_Jett.txt (72416 bytes)\n",
      "Saved: Joe_Cocker.txt (45409 bytes)\n",
      "Saved: Joe_Satriani.txt (58899 bytes)\n",
      "Saved: Joe_Walsh.txt (75568 bytes)\n",
      "Saved: John_Fogerty.txt (56672 bytes)\n",
      "Saved: John_Lennon.txt (189482 bytes)\n",
      "Saved: John_Mayall.txt (40882 bytes)\n",
      "Saved: John_Mayall_&_the_Bluesbreakers.txt (12643 bytes)\n",
      "Saved: John_Mayer.txt (177567 bytes)\n",
      "Saved: John_Mellencamp.txt (95646 bytes)\n",
      "Saved: Journey_(band).txt (85761 bytes)\n",
      "Saved: Joy_Division.txt (56531 bytes)\n",
      "Saved: Judas_Priest.txt (142491 bytes)\n",
      "Saved: Kaiser_Chiefs.txt (43828 bytes)\n",
      "Saved: Kaleo_(band).txt (41901 bytes)\n",
      "Saved: Kansas_(band).txt (48867 bytes)\n",
      "Saved: Keane_(band).txt (61322 bytes)\n",
      "Saved: Kenny_Loggins.txt (45274 bytes)\n",
      "Saved: Kid_Rock.txt (113100 bytes)\n",
      "Saved: Killswitch_Engage.txt (75350 bytes)\n",
      "Saved: Kings_of_Leon.txt (47842 bytes)\n",
      "Saved: Kiss_(band).txt (168787 bytes)\n",
      "Saved: Korn.txt (185167 bytes)\n",
      "Saved: Lacuna_Coil.txt (45461 bytes)\n",
      "Saved: Lamb_of_God_(band).txt (95482 bytes)\n",
      "Saved: Led_Zeppelin.txt (136860 bytes)\n",
      "Saved: Lenny_Kravitz.txt (85540 bytes)\n",
      "Saved: Lifehouse_(band).txt (41317 bytes)\n",
      "Saved: Limp_Bizkit.txt (190901 bytes)\n",
      "Saved: Linda_Ronstadt.txt (190765 bytes)\n",
      "Saved: Linkin_Park.txt (242868 bytes)\n",
      "Saved: Little_Richard.txt (161904 bytes)\n",
      "Saved: Little_River_Band.txt (115987 bytes)\n",
      "Saved: Live_(band).txt (36154 bytes)\n",
      "Saved: Living_Colour.txt (35978 bytes)\n",
      "Saved: Lou_Reed.txt (104387 bytes)\n",
      "Saved: Loverboy.txt (23472 bytes)\n",
      "Saved: Lynyrd_Skynyrd.txt (60358 bytes)\n",
      "Saved: Manfred_Mann.txt (26613 bytes)\n",
      "Saved: Manfred_Mann_(musician).txt (15310 bytes)\n",
      "Saved: Manfred_Mann_Chapter_Three.txt (3806 bytes)\n",
      "Saved: Manfred_Mann's_Earth_Band.txt (42779 bytes)\n",
      "Saved: Marilyn_Manson.txt (233125 bytes)\n",
      "Saved: Marilyn_Manson_(band).txt (295602 bytes)\n",
      "Saved: Matchbox_Twenty.txt (37903 bytes)\n",
      "Saved: Meat_Loaf.txt (123337 bytes)\n",
      "Saved: Megadeth.txt (191256 bytes)\n",
      "Saved: Melissa_Etheridge.txt (58332 bytes)\n",
      "Saved: Men_at_Work.txt (44833 bytes)\n",
      "Saved: Metallica.txt (220223 bytes)\n",
      "Saved: Michael_Martin_Murphey.txt (40471 bytes)\n",
      "Saved: Midnight_Oil.txt (113705 bytes)\n",
      "Saved: Mike_and_the_Mechanics.txt (26503 bytes)\n",
      "Saved: Mitch_Ryder.txt (16196 bytes)\n",
      "Saved: Modest_Mouse.txt (42352 bytes)\n",
      "Saved: Montgomery_Gentry.txt (55109 bytes)\n",
      "Saved: Morrissey.txt (191840 bytes)\n",
      "Saved: Motörhead.txt (135309 bytes)\n",
      "Saved: Mudvayne.txt (51568 bytes)\n",
      "Saved: Mumford_&_Sons.txt (70599 bytes)\n",
      "Saved: Muse_(band).txt (96531 bytes)\n",
      "Saved: My_Chemical_Romance.txt (182005 bytes)\n",
      "Saved: Mötley_Crüe.txt (189817 bytes)\n",
      "Saved: Neil_Young.txt (180559 bytes)\n",
      "Saved: New_York_Dolls.txt (66603 bytes)\n",
      "Saved: Nickelback.txt (84719 bytes)\n",
      "Saved: Nine_Inch_Nails.txt (236775 bytes)\n",
      "Saved: Nirvana_(band).txt (98301 bytes)\n",
      "Saved: No_Doubt.txt (67355 bytes)\n",
      "Saved: Oasis_(band).txt (128803 bytes)\n",
      "Saved: Our_Lady_Peace.txt (60117 bytes)\n",
      "Saved: Ozzy_Osbourne.txt (179589 bytes)\n",
      "Saved: P.O.D.txt (48241 bytes)\n",
      "Saved: Panic!_at_the_Disco.txt (135603 bytes)\n",
      "Saved: Pantera.txt (105938 bytes)\n",
      "Saved: Papa_Roach.txt (106872 bytes)\n",
      "Saved: Paramore.txt (192646 bytes)\n",
      "Saved: Parliament_(band).txt (11829 bytes)\n",
      "Saved: Parliament_-Funkadelic.txt (0 bytes)\n",
      "Saved: Pat_Benatar.txt (58387 bytes)\n",
      "Saved: Paul_McCartney.txt (265210 bytes)\n",
      "Saved: Paul_McCartney_and_Wings.txt (69214 bytes)\n",
      "Saved: Pearl_Jam.txt (133790 bytes)\n",
      "Saved: Peter_Frampton.txt (60248 bytes)\n",
      "Saved: Peter_Gabriel.txt (128308 bytes)\n",
      "Saved: Phil_Collins.txt (188385 bytes)\n",
      "Saved: Pink_Floyd.txt (187416 bytes)\n",
      "Saved: Pixies_(band).txt (94328 bytes)\n",
      "Saved: Pixies_(band).txt (94328 bytes)\n",
      "Saved: Pop_Evil.txt (23327 bytes)\n",
      "Saved: Primus_(band).txt (90641 bytes)\n",
      "Saved: Puddle_of_Mudd.txt (52868 bytes)\n",
      "Saved: Queen_(band).txt (283333 bytes)\n",
      "Saved: Queens_of_the_Stone_Age.txt (87905 bytes)\n",
      "Saved: Queensrÿche.txt (81189 bytes)\n",
      "Saved: Quiet_Riot.txt (69779 bytes)\n",
      "Saved: R.E.M.txt (120523 bytes)\n",
      "Saved: REO_Speedwagon.txt (35921 bytes)\n",
      "Saved: Radiohead.txt (217721 bytes)\n",
      "Saved: Rage_Against_the_Machine.txt (140513 bytes)\n",
      "Saved: Rainbow_(rock_band).txt (37934 bytes)\n",
      "Saved: Rammstein.txt (107174 bytes)\n",
      "Saved: Ramones.txt (118125 bytes)\n",
      "Saved: Ratt.txt (47219 bytes)\n",
      "Saved: Red_Hot_Chili_Peppers.txt (154059 bytes)\n",
      "Saved: Ringo_Starr.txt (149277 bytes)\n",
      "Saved: Rise_Against.txt (113444 bytes)\n",
      "Saved: Ritchie_Valens.txt (40657 bytes)\n",
      "Saved: Rob_Zombie.txt (128698 bytes)\n",
      "Saved: Robert_Plant.txt (91275 bytes)\n",
      "Saved: Rod_Stewart.txt (133042 bytes)\n",
      "Saved: Roxy_Music.txt (61122 bytes)\n",
      "Saved: Roy_Orbison.txt (100684 bytes)\n",
      "Saved: Royal_Blood_(band).txt (33887 bytes)\n",
      "Saved: Run-DMC.txt (54775 bytes)\n",
      "Saved: Rush_(band).txt (169187 bytes)\n",
      "Saved: Saliva_(band).txt (40953 bytes)\n",
      "Saved: Sam_Fender.txt (169838 bytes)\n",
      "Saved: Santana_(band).txt (48836 bytes)\n",
      "Saved: Saving_Abel.txt (14998 bytes)\n",
      "Saved: Scorpions_(band).txt (69156 bytes)\n",
      "Saved: Seether.txt (46452 bytes)\n",
      "Saved: Sepultura.txt (96508 bytes)\n",
      "Saved: Sex_Pistols.txt (115633 bytes)\n",
      "Saved: Shakin'_Stevens.txt (26537 bytes)\n",
      "Saved: Sheryl_Crow.txt (89021 bytes)\n",
      "Saved: Shinedown.txt (81561 bytes)\n",
      "Saved: Silverchair.txt (101071 bytes)\n",
      "Saved: Simon_&_Garfunkel.txt (83247 bytes)\n",
      "Saved: Simple_Minds.txt (111454 bytes)\n",
      "Saved: Simple_Plan.txt (63266 bytes)\n",
      "Saved: Sixto_Rodriguez.txt (49609 bytes)\n",
      "Saved: Skid_Row_(American_band).txt (53311 bytes)\n",
      "Saved: Skillet_(band).txt (77661 bytes)\n",
      "Saved: Slade.txt (126440 bytes)\n",
      "Saved: Slayer.txt (163278 bytes)\n",
      "Saved: Slipknot_(band).txt (149687 bytes)\n",
      "Saved: Sly_and_the_Family_Stone.txt (58443 bytes)\n",
      "Saved: Small_Faces.txt (50561 bytes)\n",
      "Saved: Smash_Mouth.txt (45225 bytes)\n",
      "Saved: Smokie_(band).txt (29809 bytes)\n",
      "Saved: Snow_Patrol.txt (67050 bytes)\n",
      "Saved: Social_Distortion.txt (56789 bytes)\n",
      "Saved: Sonic_Youth.txt (78130 bytes)\n",
      "Saved: Soundgarden.txt (131395 bytes)\n",
      "Saved: Staind.txt (47546 bytes)\n",
      "Saved: Starset.txt (52669 bytes)\n",
      "Saved: Starship_(band).txt (37311 bytes)\n",
      "Saved: Status_Quo_(band).txt (64602 bytes)\n",
      "Saved: Steely_Dan.txt (65270 bytes)\n",
      "Saved: Steppenwolf_(band).txt (24756 bytes)\n",
      "Saved: Steve_Miller_Band.txt (27682 bytes)\n",
      "Saved: Steve_Vai.txt (111608 bytes)\n",
      "Saved: Steve_Winwood.txt (54866 bytes)\n",
      "Saved: Stevie_Nicks.txt (144377 bytes)\n",
      "Saved: Stevie_Ray_Vaughan.txt (101998 bytes)\n",
      "Saved: Sting_(musician).txt (117134 bytes)\n",
      "Saved: Stone_Sour.txt (43107 bytes)\n",
      "Saved: Stone_Temple_Pilots.txt (66029 bytes)\n",
      "Saved: Styx_(band).txt (55443 bytes)\n",
      "Saved: Sublime_(band).txt (48477 bytes)\n",
      "Saved: Sublime_with_Rome.txt (36884 bytes)\n",
      "Saved: Sum_41.txt (203207 bytes)\n",
      "Saved: Supertramp.txt (52179 bytes)\n",
      "Saved: Survivor_(band).txt (45534 bytes)\n",
      "Saved: System_of_a_Down.txt (115439 bytes)\n",
      "Saved: T._Rex_(band).txt (63916 bytes)\n",
      "Saved: Talking_Heads.txt (55323 bytes)\n",
      "Saved: Ted_Nugent.txt (112802 bytes)\n",
      "Saved: Ten_Years_After.txt (18259 bytes)\n",
      "Saved: Tenacious_D.txt (116307 bytes)\n",
      "Saved: Tesla_(band).txt (29925 bytes)\n",
      "Saved: The_Alan_Parsons_Project.txt (23041 bytes)\n",
      "Saved: The_All_-American_Rejects.txt (0 bytes)\n",
      "Saved: The_Allman_Brothers_Band.txt (81341 bytes)\n",
      "Saved: The_Animals.txt (30597 bytes)\n",
      "Saved: The_All_-American_Rejects.txt (0 bytes)\n",
      "Saved: The_Band.txt (87308 bytes)\n",
      "Saved: The_Bangles.txt (35576 bytes)\n",
      "Saved: The_Beach_Boys.txt (233622 bytes)\n",
      "Saved: The_Beatles.txt (226130 bytes)\n",
      "Saved: The_Big_Bopper.txt (30819 bytes)\n",
      "Saved: The_Black_Crowes.txt (38893 bytes)\n",
      "Saved: The_Black_Keys.txt (112129 bytes)\n",
      "Saved: The_Boxtones.txt (15303 bytes)\n",
      "Saved: The_Byrds.txt (179362 bytes)\n",
      "Saved: The_Cab.txt (53232 bytes)\n",
      "Saved: The_Cardigans.txt (26135 bytes)\n",
      "Saved: The_Cars.txt (26304 bytes)\n",
      "Saved: The_Clash.txt (95909 bytes)\n",
      "Saved: The_Cranberries.txt (124596 bytes)\n",
      "Saved: The_Cult.txt (90817 bytes)\n",
      "Saved: The_Cure.txt (102425 bytes)\n",
      "Saved: The_Dave_Clark_Five.txt (24815 bytes)\n",
      "Saved: The_Doobie_Brothers.txt (71808 bytes)\n",
      "Saved: The_Doors.txt (99930 bytes)\n",
      "Saved: The_Everly_Brothers.txt (56116 bytes)\n",
      "Saved: The_Fixx.txt (11763 bytes)\n",
      "Saved: The_Flaming_Lips.txt (72349 bytes)\n",
      "Saved: The_Four_Seasons_(band).txt (90652 bytes)\n",
      "Saved: The_Fray.txt (55892 bytes)\n",
      "Saved: The_Go_-Go's.txt (0 bytes)\n",
      "Saved: The_Guess_Who.txt (48528 bytes)\n",
      "Saved: The_Hives.txt (38807 bytes)\n",
      "Saved: The_Hollies.txt (39560 bytes)\n",
      "Saved: The_J._Geils_Band.txt (31188 bytes)\n",
      "Saved: The_Jam.txt (52808 bytes)\n",
      "Saved: The_Jesus_and_Mary_Chain.txt (58195 bytes)\n",
      "Saved: The_Killers.txt (126722 bytes)\n",
      "Saved: The_Kinks.txt (121783 bytes)\n",
      "Saved: The_Lovin'_Spoonful.txt (167026 bytes)\n",
      "Saved: The_Lumineers.txt (61436 bytes)\n",
      "Saved: The_Mamas_&_the_Papas.txt (51499 bytes)\n",
      "Saved: The_Marshall_Tucker_Band.txt (48868 bytes)\n",
      "Saved: The_Monkees.txt (109676 bytes)\n",
      "Saved: The_Moody_Blues.txt (86276 bytes)\n",
      "Saved: The_Mothers_of_Invention.txt (55876 bytes)\n",
      "Saved: The_Offspring.txt (93836 bytes)\n",
      "Saved: The_Outfield.txt (39564 bytes)\n",
      "Saved: The_Police.txt (80387 bytes)\n",
      "Saved: The_Presidents_of_the_United_States_of_America_(band).txt (26569 bytes)\n",
      "Saved: The_Pretenders.txt (29209 bytes)\n",
      "Saved: The_Pretty_Reckless.txt (87956 bytes)\n",
      "Saved: The_Rolling_Stones.txt (269857 bytes)\n",
      "Saved: The_Script.txt (43119 bytes)\n",
      "Saved: The_Smashing_Pumpkins.txt (140200 bytes)\n",
      "Saved: The_Smiths.txt (97491 bytes)\n",
      "Saved: The_Stone_Roses.txt (62607 bytes)\n",
      "Saved: The_Strokes.txt (66845 bytes)\n",
      "Saved: The_Sweet.txt (65098 bytes)\n",
      "Saved: The_Tragically_Hip.txt (65076 bytes)\n",
      "Saved: The_Used.txt (52490 bytes)\n",
      "Saved: The_Velvet_Underground.txt (93903 bytes)\n",
      "Saved: The_Verve.txt (46826 bytes)\n",
      "Saved: The_White_Stripes.txt (176590 bytes)\n",
      "Saved: The_Who.txt (179933 bytes)\n",
      "Saved: The_Yardbirds.txt (55145 bytes)\n",
      "Saved: The_Zombies.txt (57849 bytes)\n",
      "Saved: Theory_of_a_Deadman.txt (39638 bytes)\n",
      "Saved: Thin_Lizzy.txt (87642 bytes)\n",
      "Saved: Third_Eye_Blind.txt (86355 bytes)\n",
      "Saved: Thirty_Seconds_to_Mars.txt (67074 bytes)\n",
      "Saved: Thousand_Foot_Krutch.txt (45676 bytes)\n",
      "Saved: Three_Days_Grace.txt (126728 bytes)\n",
      "Saved: Three_Dog_Night.txt (38512 bytes)\n",
      "Saved: Tom_Petty.txt (95068 bytes)\n",
      "Saved: Tom_Petty_and_the_Heartbreakers.txt (42177 bytes)\n",
      "Saved: Tommy_James_and_the_Shondells.txt (26170 bytes)\n",
      "Saved: Tool_(band).txt (138031 bytes)\n",
      "Saved: Toto_(band).txt (70162 bytes)\n",
      "Saved: Tracy_Chapman.txt (44145 bytes)\n",
      "Saved: Traffic_(band).txt (30927 bytes)\n",
      "Saved: Train_(band).txt (75411 bytes)\n",
      "Saved: Traveling_Wilburys.txt (63434 bytes)\n",
      "Saved: Travis_(band).txt (45718 bytes)\n",
      "Saved: Trivium_(band).txt (89176 bytes)\n",
      "Saved: Twenty_One_Pilots.txt (198286 bytes)\n",
      "Saved: Twisted_Sister.txt (44930 bytes)\n",
      "Saved: U2.txt (263343 bytes)\n",
      "Saved: UB40.txt (68172 bytes)\n",
      "Saved: Uriah_Heep_(band).txt (67146 bytes)\n",
      "Saved: Vampire_Weekend.txt (65996 bytes)\n",
      "Saved: Van_Halen.txt (107232 bytes)\n",
      "Saved: Van_Morrison.txt (201085 bytes)\n",
      "Saved: Tommy_James_and_the_Shondells.txt (26170 bytes)\n",
      "Saved: Velvet_Revolver.txt (68154 bytes)\n",
      "Saved: Volbeat.txt (40721 bytes)\n",
      "Saved: W.A.S.P._(band).txt (38690 bytes)\n",
      "Saved: Warrant_(American_band).txt (30944 bytes)\n",
      "Saved: Weezer.txt (169413 bytes)\n",
      "Saved: White_Zombie_(band).txt (28803 bytes)\n",
      "Saved: Whitesnake.txt (175169 bytes)\n",
      "Saved: X_(American_band).txt (32895 bytes)\n",
      "Saved: X_Ambassadors.txt (36635 bytes)\n",
      "Saved: Yes_(band).txt (133181 bytes)\n",
      "Saved: ZZ_Top.txt (64601 bytes)\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, pathlib, requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "INPUT_LIST = \"rock_artists.txt\"      # one title per line\n",
    "OUT_DIR     = \"pages_raw_wikitext\"   # output folder\n",
    "SLEEP_SEC   = 0.2                    # gentle rate limit\n",
    "\n",
    "API = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# MediaWiki lets up to ~50 titles per request for 'query'\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "def safe_filename(name: str, maxlen: int = 150) -> str:\n",
    "    \"\"\"\n",
    "    Make a Windows-safe filename from a page title.\n",
    "    Replaces / \\ : * ? \" < > | with underscores, trims spaces/dots.\n",
    "    \"\"\"\n",
    "    name = re.sub(r'[\\\\/:*?\"<>|]+', \"_\", name)\n",
    "    name = name.strip(\" .\")\n",
    "    if len(name) > maxlen:\n",
    "        name = name[:maxlen]\n",
    "    return name\n",
    "\n",
    "def to_api_title(title):\n",
    "    return title.strip().replace(\" \", \"_\")\n",
    "\n",
    "def load_titles(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        titles = [line.strip() for line in f if line.strip()]\n",
    "    # Deduplicate but preserve order\n",
    "    seen, clean = set(), []\n",
    "    for t in titles:\n",
    "        norm = to_api_title(t)\n",
    "        if norm not in seen:\n",
    "            clean.append(norm)\n",
    "            seen.add(norm)\n",
    "    return clean\n",
    "\n",
    "def batched(iterable, n):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]\n",
    "\n",
    "def fetch_wikitext_batch(titles):\n",
    "    \"\"\"\n",
    "    Fetch wikitext via action=query&prop=revisions&rvslots=main&rvprop=content\n",
    "    Handles redirects. Returns dict: {normalized_title: {\"title\":..., \"pageid\":..., \"wikitext\":...}}\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"redirects\": 1,\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"formatversion\": 2,\n",
    "        \"titles\": \"|\".join(titles),\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"RockNetworkBot/0.1 (https://github.com/yourusername; your_email@example.com)\"\n",
    "    }\n",
    "    r = requests.get(API, params=params, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    ...\n",
    "\n",
    "\n",
    "    # Build normalization/redirect mapping from response (if present)\n",
    "    title_map = {t: t for t in titles}\n",
    "    for norm in data.get(\"query\", {}).get(\"normalized\", []) or []:\n",
    "        title_map[norm[\"from\"].replace(\" \", \"_\")] = norm[\"to\"].replace(\" \", \"_\")\n",
    "    for redir in data.get(\"query\", {}).get(\"redirects\", []) or []:\n",
    "        title_map[redir[\"from\"].replace(\" \", \"_\")] = redir[\"to\"].replace(\" \", \"_\")\n",
    "\n",
    "    out = {}\n",
    "    for page in data.get(\"query\", {}).get(\"pages\", []):\n",
    "        title = page.get(\"title\", \"\").replace(\" \", \"_\")\n",
    "        pageid = page.get(\"pageid\", None)\n",
    "        revs = page.get(\"revisions\", [])\n",
    "        wikitext = \"\"\n",
    "        if revs and \"slots\" in revs[0] and \"main\" in revs[0][\"slots\"]:\n",
    "            wikitext = revs[0][\"slots\"][\"main\"].get(\"content\", \"\")\n",
    "\n",
    "        out[title] = {\n",
    "            \"title\": title,\n",
    "            \"pageid\": pageid,\n",
    "            \"wikitext\": wikitext,\n",
    "            \"missing\": page.get(\"missing\", False)\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    titles = load_titles(INPUT_LIST)\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    for chunk in batched(titles, BATCH_SIZE):\n",
    "        try:\n",
    "            data = fetch_wikitext_batch(chunk)\n",
    "        except Exception as e:\n",
    "            print(\"Batch failed:\", e)\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "\n",
    "        for title in chunk:\n",
    "            # Resolve final title if API redirected/normalized it\n",
    "            # We’ll choose the returned key if present\n",
    "            # Fall back to the original requested title\n",
    "            saved = None\n",
    "            # Prefer exact returned match if found\n",
    "            for k in (title,) + tuple(data.keys()):\n",
    "                if k in data:\n",
    "                    saved = data[k]\n",
    "                    break\n",
    "            if not saved:\n",
    "                # Nothing came back—skip\n",
    "                print(f\"[WARN] No data for {title}\")\n",
    "                continue\n",
    "\n",
    "            final_title = saved[\"title\"] or title\n",
    "            safe_title  = safe_filename(final_title)   # sanitize it\n",
    "            fname = f\"{safe_title}.txt\"\n",
    "            fpath = pathlib.Path(OUT_DIR) / fname\n",
    "\n",
    "            # Save raw wikitext; also drop a tiny JSON sidecar with metadata\n",
    "            with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(saved.get(\"wikitext\", \"\"))\n",
    "\n",
    "            meta = {\n",
    "                \"requested_title\": title,\n",
    "                \"final_title\": final_title,\n",
    "                \"pageid\": saved.get(\"pageid\"),\n",
    "                \"missing\": saved.get(\"missing\", False),\n",
    "                \"bytes\": len(saved.get(\"wikitext\", \"\")),\n",
    "            }\n",
    "            with open(str(fpath) + \".json\", \"w\", encoding=\"utf-8\") as jf:\n",
    "                json.dump(meta, jf, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"Saved: {fname} ({meta['bytes']} bytes)\")\n",
    "        time.sleep(SLEEP_SEC)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844690d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolates: ['Van_Zant_(band)', 'Dr._Hook_&_the_Medicine_Show', 'Jet_(Australian_band)']\n",
      "Nodes: 486\n",
      "Edges: 8378\n",
      "AC/DC in-degree: 41\n",
      "AC/DC out-degree: 34\n",
      "Black_Sabbath in-degree: 74\n",
      "Black_Sabbath out-degree: 64\n"
     ]
    }
   ],
   "source": [
    "# build_rock_network.py\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "import networkx as nx\n",
    "\n",
    "PAGES_DIR = pathlib.Path(\"pages_raw_wikitext\")\n",
    "ARTISTS_TXT = \"rock_artists.txt\"\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "# Capture [[Target]] and [[Target|display]], ignoring section anchors (after #)\n",
    "LINK_RE = re.compile(r\"\\[\\[([^\\[\\]\\|#]+)(?:\\|[^\\[\\]]+)?\\]\\]\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ']+\")\n",
    "\n",
    "# NEW/CHANGED: normalization regexes\n",
    "DASH_RE = re.compile(r\"\\s*[\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015-]\\s*\")  # all common dash chars\n",
    "APOST_RE = re.compile(r\"[’‘`´]\")  # curly/smart apostrophes & accents -> '\n",
    "AND_TOKEN_RE = re.compile(r\"\\s+(?:&|and)\\s+\", flags=re.IGNORECASE)  # unify &/and during variants\n",
    "\n",
    "def normalize_basic(s: str) -> str:\n",
    "    \"\"\"Normalize punctuation and spacing so variant lookups become reliable.\"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)   # collapse whitespace\n",
    "    s = APOST_RE.sub(\"'\", s)     # unify apostrophes\n",
    "    s = DASH_RE.sub(\"-\", s)      # unify dashes and trim spaces around them\n",
    "    return s\n",
    "\n",
    "def norm_key(s: str) -> str:\n",
    "    \"\"\"Key used for the variant index: normalized + lowercased.\"\"\"\n",
    "    return normalize_basic(s).lower() if s else s\n",
    "\n",
    "def extract_links_from_wikitext(text: str):\n",
    "    \"\"\"Return list of raw wiki targets e.g. 'Black Sabbath' or 'AC/DC' (spaces preserved here).\"\"\"\n",
    "    return [m.group(1).strip() for m in LINK_RE.finditer(text)]\n",
    "\n",
    "def canon_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Canonical node label:\n",
    "    - Normalize punctuation/spacing\n",
    "    - Replace spaces with underscores\n",
    "    - KEEP slashes (e.g., AC/DC stays AC/DC)\n",
    "    \"\"\"\n",
    "    t = normalize_basic(title)  # NEW/CHANGED\n",
    "    return t.replace(\"  \", \" \").replace(\" \", \"_\")\n",
    "\n",
    "# NEW/CHANGED: helper to generate &/and forms\n",
    "def amp_variants(s: str):\n",
    "    base = normalize_basic(s)\n",
    "    with_amp = AND_TOKEN_RE.sub(\" & \", base)\n",
    "    with_and = AND_TOKEN_RE.sub(\" and \", base)\n",
    "    return {base, with_amp, with_and}\n",
    "\n",
    "def variants_for_lookup(title: str):\n",
    "    \"\"\"\n",
    "    Variants to help map filenames/links to canonical performer titles.\n",
    "    We include: spaces, underscores, slashes, &/and forms, and lowercase.\n",
    "    (We still store them via norm_key, so case differences are collapsed.)\n",
    "    \"\"\"\n",
    "    variants = set()\n",
    "    for seed in amp_variants(title):\n",
    "        # start from normalized seed\n",
    "        seed = normalize_basic(seed)\n",
    "        t_space = seed.replace(\"_\", \" \")\n",
    "        t_under = seed.replace(\" \", \"_\")\n",
    "        # swap in slashes from both space and underscore forms\n",
    "        t_slash_from_under = t_under.replace(\"_\", \"/\")\n",
    "        t_slash_from_space = t_space.replace(\" \", \"/\")\n",
    "\n",
    "        variants.update({\n",
    "            seed, t_space, t_under, t_slash_from_under, t_slash_from_space,\n",
    "            seed.lower(), t_space.lower(), t_under.lower(),\n",
    "            t_slash_from_under.lower(), t_slash_from_space.lower(),\n",
    "        })\n",
    "\n",
    "        # also include minor hyphen/space variants (after dash normalization this is small)\n",
    "        variants.add(seed.replace(\"-\", \" - \"))\n",
    "        variants.add(seed.replace(\" - \", \"-\"))\n",
    "\n",
    "    # final pass: normalize again so weird combos collapse\n",
    "    return {normalize_basic(v) for v in variants}\n",
    "\n",
    "def build_performer_index(performers_set):\n",
    "    \"\"\"\n",
    "    Build a mapping from many variants -> canonical performer title (canonical uses underscores, keeps slashes).\n",
    "    Uses norm_key() so lookups are consistent.\n",
    "    \"\"\"\n",
    "    idx = {}\n",
    "    for p in performers_set:\n",
    "        for v in variants_for_lookup(p):\n",
    "            idx[norm_key(v)] = p  # NEW/CHANGED\n",
    "    return idx\n",
    "\n",
    "def count_words(text: str) -> int:\n",
    "    return len(WORD_RE.findall(text))\n",
    "\n",
    "# -------------------------\n",
    "# Load performers\n",
    "# -------------------------\n",
    "with open(ARTISTS_TXT, encoding=\"utf-8\") as f:\n",
    "    # Canonicalize with normalization: replace spaces with underscores; keep slashes in names like AC/DC\n",
    "    performers = {canon_title(line) for line in f if line.strip()}  # canon_title() now normalizes\n",
    "\n",
    "# Build a variant index for robust matching\n",
    "index = build_performer_index(performers)\n",
    "\n",
    "# -------------------------\n",
    "# Build the directed graph\n",
    "# -------------------------\n",
    "G = nx.DiGraph()\n",
    "for p in performers:\n",
    "    # initialize nodes with a placeholder word count\n",
    "    G.add_node(p, words=0, title=p.replace(\"_\", \" \"))  # store a human-readable title as well\n",
    "\n",
    "# Iterate saved pages (*.txt)\n",
    "for txt_path in PAGES_DIR.glob(\"*.txt\"):\n",
    "    # 1) Determine the true wiki title for this file (prefer sidecar .json if exists)\n",
    "    meta_path = txt_path.with_suffix(\".txt.json\")\n",
    "    if meta_path.exists():\n",
    "        meta = json.loads(meta_path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "        final_title = meta.get(\"final_title\") or txt_path.stem\n",
    "    else:\n",
    "        final_title = txt_path.stem\n",
    "\n",
    "    # Canonicalize the source node label (normalize + spaces->underscores, keep slashes)\n",
    "    src_canon = canon_title(final_title)\n",
    "\n",
    "    # If not directly known, try variant mapping using filename stem as well\n",
    "    if src_canon not in performers:\n",
    "        # try a few variants from stem and final_title to find a performer\n",
    "        candidates = set()\n",
    "        for raw in {txt_path.stem, final_title, src_canon}:\n",
    "            candidates |= variants_for_lookup(raw)\n",
    "            candidates |= variants_for_lookup(canon_title(raw))\n",
    "        # NEW/CHANGED: index lookup via norm_key\n",
    "        src_mapped = next((index.get(norm_key(v)) for v in candidates if norm_key(v) in index), None)\n",
    "        if not src_mapped:\n",
    "            # couldn't map this file to a performer in your list\n",
    "            continue\n",
    "        src = src_mapped\n",
    "    else:\n",
    "        src = src_canon\n",
    "\n",
    "    # 2) Read wikitext and set node word count\n",
    "    text = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    G.nodes[src][\"words\"] = count_words(text)\n",
    "\n",
    "    # 3) Extract outgoing link targets from wikitext\n",
    "    raw_links = extract_links_from_wikitext(text)\n",
    "\n",
    "    # 4) Normalize and add edges only if target is another performer\n",
    "    for tgt_raw in raw_links:\n",
    "        # Prefer canonicalization path first\n",
    "        tgt_canon_guess = canon_title(tgt_raw)  # canon_title() includes normalization now\n",
    "\n",
    "        if tgt_canon_guess in performers:\n",
    "            tgt = tgt_canon_guess\n",
    "        else:\n",
    "            # NEW/CHANGED: single consistent lookup key\n",
    "            tgt = index.get(norm_key(tgt_raw))\n",
    "\n",
    "        if tgt and tgt != src and tgt in performers:\n",
    "            G.add_edge(src, tgt)\n",
    "\n",
    "# -------------------------\n",
    "\n",
    "# Prune isolates and keep largest weakly connected component\n",
    "# -------------------------\n",
    "isolates = list(nx.isolates(G))\n",
    "print(\"Isolates:\", isolates)\n",
    "G.remove_nodes_from(isolates)\n",
    "\n",
    "if G.number_of_nodes() > 0:\n",
    "    lcc_nodes = max(nx.weakly_connected_components(G), key=len)\n",
    "    G_lcc = G.subgraph(lcc_nodes).copy()\n",
    "else:\n",
    "    G_lcc = G\n",
    "\n",
    "# -------------------------\n",
    "# Inspect and save\n",
    "# -------------------------\n",
    "print(\"Nodes:\", G_lcc.number_of_nodes())\n",
    "print(\"Edges:\", G_lcc.number_of_edges())\n",
    "\n",
    "# Quick spot check for AC/DC and Black Sabbath if present\n",
    "for band in (\"AC/DC\", \"Black_Sabbath\"):\n",
    "    b = canon_title(band)\n",
    "    if b in G_lcc:\n",
    "        print(f\"{band} in-degree:\", G_lcc.in_degree(b))\n",
    "        print(f\"{band} out-degree:\", G_lcc.out_degree(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea2020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-neighbors: [\"Guns_N'_Roses\", 'The_Velvet_Underground', 'The_Rolling_Stones', 'Chuck_Berry', 'The_Beatles', 'Lou_Reed', 'Deep_Purple', 'Black_Sabbath', 'Ozzy_Osbourne', 'Ted_Nugent']\n",
      "In-neighbors: ['Volbeat', 'Foreigner_(band)', 'Marilyn_Manson_(band)', 'Boston_(band)', 'Megadeth', 'My_Chemical_Romance', \"Guns_N'_Roses\", 'Sum_41', 'Sepultura', 'The_Cult']\n"
     ]
    }
   ],
   "source": [
    "band = \"AC/DC\"\n",
    "print(\"Out-neighbors:\", list(G_lcc.successors(band))[:10])  # pages it links TO\n",
    "print(\"In-neighbors:\", list(G_lcc.predecessors(band))[:10]) # pages that link TO it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3891f164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malin Haugland Høli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\readwrite\\json_graph\\node_link.py:145: FutureWarning: \n",
      "The default value will be `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "\n",
    "# GEXF (good for Gephi, Cytoscape, and reloading into NetworkX)\n",
    "nx.write_gexf(G_lcc, \"rock_network.gexf\")\n",
    "\n",
    "# GraphML (also widely supported)\n",
    "nx.write_graphml(G_lcc, \"rock_network.graphml\")\n",
    "\n",
    "# Or JSON (node-link format)\n",
    "with open(\"rock_network.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_graph.node_link_data(G_lcc), f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077aab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
