{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f36f326048cdf5",
   "metadata": {},
   "source": [
    "## Rock Music Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af50f62",
   "metadata": {},
   "source": [
    "#### *Step 1: Get a clean list of page titles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5274079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Parsing titles...\n",
      "Saved 489 titles to rock_artists.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "PDF_PATH = \"rock_artists.pdf\"\n",
    "TXT_EXPORT = \"rock_artists_raw.txt\"\n",
    "OUT_LIST = \"rock_artists.txt\"\n",
    "\n",
    "def pdf_to_text(pdf_path, txt_path):\n",
    "    import PyPDF2\n",
    "    with open(pdf_path, \"rb\") as f, open(txt_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            out.write(page.extract_text() or \"\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "def parse_titles(text):\n",
    "    titles = set()\n",
    "\n",
    "    # Preferred: wiki-style [[Title]] or [[Title|display]]\n",
    "    for raw in re.findall(r\"\\[\\[([^\\[\\]]+)\\]\\]\", text):\n",
    "        left = raw.split(\"|\", 1)[0].strip()\n",
    "        if not left or \":\" in left:  # skip namespaces like Category:\n",
    "            continue\n",
    "        titles.add(left)\n",
    "\n",
    "    if not titles:\n",
    "        # Fallback: lines that look like Proper Noun titles (very heuristic)\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if 2 <= len(line) <= 120 and line[0].isupper() and not line.endswith(\":\"):\n",
    "                # crude filter to avoid headers; adjust as needed\n",
    "                titles.add(line)\n",
    "\n",
    "    # Normalize: keep canonical version with spaces; downloader will add underscores\n",
    "    clean = []\n",
    "    seen = set()\n",
    "    for t in titles:\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "        if t and t not in seen:\n",
    "            seen.add(t)\n",
    "            clean.append(t)\n",
    "    clean.sort()\n",
    "    return clean\n",
    "\n",
    "def main():\n",
    "    if not Path(PDF_PATH).exists():\n",
    "        raise SystemExit(f\"PDF not found: {PDF_PATH}. Put your PDF next to this script.\")\n",
    "\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    pdf_to_text(PDF_PATH, TXT_EXPORT)\n",
    "    text = Path(TXT_EXPORT).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    print(\"Parsing titles...\")\n",
    "    titles = parse_titles(text)\n",
    "    if not titles:\n",
    "        raise SystemExit(\"Found 0 titles. Open rock_artists_raw.txt and check the formatting.\")\n",
    "\n",
    "    Path(OUT_LIST).write_text(\"\\n\".join(titles), encoding=\"utf-8\")\n",
    "    print(f\"Saved {len(titles)} titles to {OUT_LIST}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fab465",
   "metadata": {},
   "source": [
    "#### *Step 2: Fetch raw Wikipedia wikitext for each title*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14a71a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 10_Years_(band).txt (28465 bytes)\n",
      "Saved: 10cc.txt (67283 bytes)\n",
      "Saved: 3_Doors_Down.txt (46731 bytes)\n",
      "Saved: 311_(band).txt (55903 bytes)\n",
      "Saved: 38_Special_(band).txt (29277 bytes)\n",
      "Saved: A_Perfect_Circle.txt (89175 bytes)\n",
      "Saved: ABBA.txt (152299 bytes)\n",
      "Saved: AC_DC.txt (184056 bytes)\n",
      "Saved: AFI_(band).txt (61575 bytes)\n",
      "Saved: Accept_(band).txt (47788 bytes)\n",
      "Saved: Adam_Ant.txt (94230 bytes)\n",
      "Saved: Aerosmith.txt (192561 bytes)\n",
      "Saved: Air_Supply.txt (52266 bytes)\n",
      "Saved: Alanis_Morissette.txt (122824 bytes)\n",
      "Saved: Alice_Cooper.txt (181022 bytes)\n",
      "Saved: Alice_Cooper_(band).txt (39506 bytes)\n",
      "Saved: Alice_in_Chains.txt (205052 bytes)\n",
      "Saved: AllMusic.txt (13197 bytes)\n",
      "Saved: Alter_Bridge.txt (78955 bytes)\n",
      "Saved: Ambrosia_(band).txt (30410 bytes)\n",
      "Saved: America_(band).txt (52835 bytes)\n",
      "Saved: Anthrax_(American_band).txt (93547 bytes)\n",
      "Saved: April_Wine.txt (37365 bytes)\n",
      "Saved: Arcade_Fire.txt (106135 bytes)\n",
      "Saved: Arctic_Monkeys.txt (159436 bytes)\n",
      "Saved: Asia_(band).txt (59659 bytes)\n",
      "Saved: Audioslave.txt (87821 bytes)\n",
      "Saved: Avenged_Sevenfold.txt (106624 bytes)\n",
      "Saved: Avril_Lavigne.txt (246960 bytes)\n",
      "Saved: Awolnation.txt (59975 bytes)\n",
      "Saved: Bachman_–Turner_Overdrive.txt (0 bytes)\n",
      "Saved: Bad_Company.txt (34769 bytes)\n",
      "Saved: Badfinger.txt (93639 bytes)\n",
      "Saved: Barenaked_Ladies.txt (82705 bytes)\n",
      "Saved: Bay_City_Rollers.txt (79860 bytes)\n",
      "Saved: Beastie_Boys.txt (130143 bytes)\n",
      "Saved: Beck.txt (122512 bytes)\n",
      "Saved: Bee_Gees.txt (143345 bytes)\n",
      "Saved: Ben_Folds_Five.txt (25531 bytes)\n",
      "Saved: Bill_Haley_&_His_Comets.txt (62159 bytes)\n",
      "Saved: Billy_Idol.txt (74121 bytes)\n",
      "Saved: Billy_Joel.txt (181050 bytes)\n",
      "Saved: Billy_Squier.txt (28033 bytes)\n",
      "Saved: Billy_Talent.txt (49055 bytes)\n",
      "Saved: Black_Sabbath.txt (188798 bytes)\n",
      "Saved: Black_Stone_Cherry.txt (19796 bytes)\n",
      "Saved: Black_Veil_Brides.txt (96273 bytes)\n",
      "Saved: Blink-182.txt (141743 bytes)\n",
      "Saved: Blondie_(band).txt (77330 bytes)\n",
      "Saved: Bloodhound_Gang.txt (32886 bytes)\n",
      "Saved: Blue_October.txt (39178 bytes)\n",
      "Saved: Blue_Öyster_Cult.txt (68238 bytes)\n",
      "Saved: Blues_Traveler.txt (33127 bytes)\n",
      "Saved: Blur_(band).txt (81508 bytes)\n",
      "Saved: Bo_Diddley.txt (81418 bytes)\n",
      "Saved: Bob_Dylan.txt (278031 bytes)\n",
      "Saved: Bob_Seger.txt (50393 bytes)\n",
      "Saved: Bon_Jovi.txt (89727 bytes)\n",
      "Saved: Boston_(band).txt (53085 bytes)\n",
      "Saved: Bowling_for_Soup.txt (66415 bytes)\n",
      "Saved: Boys_Like_Girls.txt (61385 bytes)\n",
      "Saved: Bread_(band).txt (38281 bytes)\n",
      "Saved: Breaking_Benjamin.txt (109076 bytes)\n",
      "Saved: Bring_Me_the_Horizon.txt (197739 bytes)\n",
      "Saved: Bruce_Springsteen.txt (218222 bytes)\n",
      "Saved: Bryan_Adams.txt (248102 bytes)\n",
      "Saved: Bryan_Ferry.txt (73863 bytes)\n",
      "Saved: Buckcherry.txt (31717 bytes)\n",
      "Saved: Buddy_Holly.txt (79340 bytes)\n",
      "Saved: Bullet_for_My_Valentine.txt (123476 bytes)\n",
      "Saved: Bush_(British_band).txt (46326 bytes)\n",
      "Saved: Cage_the_Elephant.txt (62591 bytes)\n",
      "Saved: Cake_(band).txt (59354 bytes)\n",
      "Saved: Canned_Heat.txt (40199 bytes)\n",
      "Saved: Catfish_and_the_Bottlemen.txt (62179 bytes)\n",
      "Saved: Charlie_Daniels.txt (50152 bytes)\n",
      "Saved: Cheap_Trick.txt (44958 bytes)\n",
      "Saved: Chevelle_(band).txt (102084 bytes)\n",
      "Saved: Chicago_(band).txt (244793 bytes)\n",
      "Saved: Chris_Cornell.txt (218955 bytes)\n",
      "Saved: Chris_Rea.txt (68030 bytes)\n",
      "Saved: Chris_de_Burgh.txt (27668 bytes)\n",
      "Saved: Christopher_Cross.txt (43449 bytes)\n",
      "Saved: Chubby_Checker.txt (46441 bytes)\n",
      "Saved: Chuck_Berry.txt (80008 bytes)\n",
      "Saved: Cinderella_(band).txt (26250 bytes)\n",
      "Saved: Coheed_and_Cambria.txt (73044 bytes)\n",
      "Saved: Cold_Chisel.txt (74591 bytes)\n",
      "Saved: Coldplay.txt (294640 bytes)\n",
      "Saved: Collective_Soul.txt (37246 bytes)\n",
      "Saved: Counting_Crows.txt (59777 bytes)\n",
      "Saved: Crash_Test_Dummies.txt (34360 bytes)\n",
      "Saved: Cream_(band).txt (41250 bytes)\n",
      "Saved: Creed_(band).txt (78810 bytes)\n",
      "Saved: Creedence_Clearwater_Revival.txt (65218 bytes)\n",
      "Saved: Crosby,_Stills,_Nash_&_Young.txt (96987 bytes)\n",
      "Saved: Crowded_House.txt (67157 bytes)\n",
      "Saved: Buddy_Holly.txt (79340 bytes)\n",
      "Saved: Damn_Yankees_(band).txt (16329 bytes)\n",
      "Saved: Dan_Fogelberg.txt (23899 bytes)\n",
      "Saved: Dashboard_Confessional.txt (39190 bytes)\n",
      "Saved: Daughtry_(band).txt (52315 bytes)\n",
      "Saved: Dave_Matthews_Band.txt (79173 bytes)\n",
      "Saved: David_Bowie.txt (262214 bytes)\n",
      "Saved: Days_of_the_New.txt (37019 bytes)\n",
      "Saved: Death_Cab_for_Cutie.txt (64166 bytes)\n",
      "Saved: Deep_Purple.txt (141805 bytes)\n",
      "Saved: Def_Leppard.txt (102198 bytes)\n",
      "Saved: Deftones.txt (133962 bytes)\n",
      "Saved: Depeche_Mode.txt (136529 bytes)\n",
      "Saved: Dio_(band).txt (31948 bytes)\n",
      "Saved: Dire_Straits.txt (79194 bytes)\n",
      "Saved: Disturbed_(band).txt (75429 bytes)\n",
      "Saved: Don_Henley.txt (69048 bytes)\n",
      "Saved: Donovan.txt (74659 bytes)\n",
      "Saved: Depeche_Mode.txt (136529 bytes)\n",
      "Saved: Dropkick_Murphys.txt (89170 bytes)\n",
      "Saved: Drowning_Pool.txt (36692 bytes)\n",
      "Saved: Duane_Eddy.txt (63626 bytes)\n",
      "Saved: Duran_Duran.txt (124862 bytes)\n",
      "Saved: Eagles_(band).txt (102108 bytes)\n",
      "Saved: Echo_&_the_Bunnymen.txt (47640 bytes)\n",
      "Saved: Eddie_Cochran.txt (47163 bytes)\n",
      "Saved: Eddie_Money.txt (36849 bytes)\n",
      "Saved: Edgar_Winter.txt (40690 bytes)\n",
      "Saved: Electric_Light_Orchestra.txt (59949 bytes)\n",
      "Saved: Elton_John.txt (257404 bytes)\n",
      "Saved: Elvis_Costello.txt (180544 bytes)\n",
      "Saved: Elvis_Presley.txt (215364 bytes)\n",
      "Saved: Emerson,_Lake_&_Palmer.txt (66062 bytes)\n",
      "Saved: England_Dan_&_John_Ford_Coley.txt (15383 bytes)\n",
      "Saved: Eric_Clapton.txt (196657 bytes)\n",
      "Saved: Europe_(band).txt (53497 bytes)\n",
      "Saved: Evanescence.txt (184413 bytes)\n",
      "Saved: Everclear_(band).txt (37459 bytes)\n",
      "Saved: Everlast.txt (26893 bytes)\n",
      "Saved: Extreme_(band).txt (25135 bytes)\n",
      "Saved: Faces_(band).txt (37473 bytes)\n",
      "Saved: Faith_No_More.txt (98540 bytes)\n",
      "Saved: Fall_Out_Boy.txt (232732 bytes)\n",
      "Saved: Fats_Domino.txt (53471 bytes)\n",
      "Saved: Filter_(band).txt (51416 bytes)\n",
      "Saved: Finger_Eleven.txt (38241 bytes)\n",
      "Saved: FireHouse.txt (22723 bytes)\n",
      "Saved: Five_Finger_Death_Punch.txt (105588 bytes)\n",
      "Saved: Five_for_Fighting.txt (72381 bytes)\n",
      "Saved: Fleetwood_Mac.txt (125302 bytes)\n",
      "Saved: Flogging_Molly.txt (17651 bytes)\n",
      "Saved: Florence_and_the_Machine.txt (109464 bytes)\n",
      "Saved: Flyleaf_(band).txt (48001 bytes)\n",
      "Saved: Foals_(band).txt (43037 bytes)\n",
      "Saved: Foo_Fighters.txt (123163 bytes)\n",
      "Saved: Foreigner_(band).txt (76376 bytes)\n",
      "Saved: Foster_the_People.txt (67084 bytes)\n",
      "Saved: Frank_Zappa.txt (219982 bytes)\n",
      "Saved: Franz_Ferdinand_(band).txt (54186 bytes)\n",
      "Saved: Fuel_(band).txt (35297 bytes)\n",
      "Saved: Fun_(band).txt (25203 bytes)\n",
      "Saved: Funkadelic.txt (18848 bytes)\n",
      "Saved: Garbage_(band).txt (103731 bytes)\n",
      "Saved: Gary_Glitter.txt (88858 bytes)\n",
      "Saved: Gene_Vincent.txt (45506 bytes)\n",
      "Saved: Genesis_(band).txt (156027 bytes)\n",
      "Saved: George_Harrison.txt (167207 bytes)\n",
      "Saved: George_Thorogood.txt (42262 bytes)\n",
      "Saved: Ghost_(Swedish_band).txt (165046 bytes)\n",
      "Saved: Gin_Blossoms.txt (31062 bytes)\n",
      "Saved: Glenn_Frey.txt (56283 bytes)\n",
      "Saved: Godsmack.txt (90299 bytes)\n",
      "Saved: Golden_Earring.txt (25844 bytes)\n",
      "Saved: Goo_Goo_Dolls.txt (50770 bytes)\n",
      "Saved: Good_Charlotte.txt (80166 bytes)\n",
      "Saved: Grand_Funk_Railroad.txt (33307 bytes)\n",
      "Saved: Grateful_Dead.txt (135875 bytes)\n",
      "Saved: Great_White.txt (60837 bytes)\n",
      "Saved: Green_Day.txt (218560 bytes)\n",
      "Saved: Greta_Van_Fleet.txt (61276 bytes)\n",
      "Saved: Guns_N'_Roses.txt (370109 bytes)\n",
      "Saved: Halestorm.txt (39602 bytes)\n",
      "Saved: Hall_&_Oates.txt (75095 bytes)\n",
      "Saved: Harry_Chapin.txt (42055 bytes)\n",
      "Saved: Harry_Nilsson.txt (56224 bytes)\n",
      "Saved: Heart_(band).txt (83834 bytes)\n",
      "Saved: Herman's_Hermits.txt (39892 bytes)\n",
      "Saved: Highly_Suspect.txt (32116 bytes)\n",
      "Saved: Hinder.txt (40372 bytes)\n",
      "Saved: Hole_(band).txt (104116 bytes)\n",
      "Saved: Hoobastank.txt (30366 bytes)\n",
      "Saved: Hootie_&_the_Blowfish.txt (29136 bytes)\n",
      "Saved: Huey_Lewis_and_the_News.txt (35602 bytes)\n",
      "Saved: INXS.txt (90668 bytes)\n",
      "Saved: Ian_Dury.txt (54481 bytes)\n",
      "Saved: Icehouse_(band).txt (63929 bytes)\n",
      "Saved: Iggy_Pop.txt (109374 bytes)\n",
      "Saved: Imagine_Dragons.txt (112652 bytes)\n",
      "Saved: Incubus_(band).txt (70203 bytes)\n",
      "Saved: Interpol_(band).txt (40113 bytes)\n",
      "Saved: Iron_Maiden.txt (173892 bytes)\n",
      "Saved: Jack_White.txt (143960 bytes)\n",
      "Saved: Jackson_Browne.txt (54126 bytes)\n",
      "Saved: James_Blunt.txt (75081 bytes)\n",
      "Saved: James_Taylor.txt (90594 bytes)\n",
      "Saved: Jane's_Addiction.txt (90417 bytes)\n",
      "Saved: Janis_Joplin.txt (142969 bytes)\n",
      "Saved: Jeff_Buckley.txt (85360 bytes)\n",
      "Saved: Jefferson_Airplane.txt (83595 bytes)\n",
      "Saved: Jefferson_Starship.txt (66679 bytes)\n",
      "Saved: Jerry_Lee_Lewis.txt (94328 bytes)\n",
      "Saved: John_Lennon.txt (190580 bytes)\n",
      "Saved: Jethro_Tull_(band).txt (77465 bytes)\n",
      "Saved: Jim_Croce.txt (28859 bytes)\n",
      "Saved: Jimi_Hendrix.txt (179540 bytes)\n",
      "Saved: Jimmy_Eat_World.txt (50303 bytes)\n",
      "Saved: Joan_Jett.txt (72451 bytes)\n",
      "Saved: Joe_Cocker.txt (45886 bytes)\n",
      "Saved: Joe_Satriani.txt (58812 bytes)\n",
      "Saved: Joe_Walsh.txt (75602 bytes)\n",
      "Saved: John_Fogerty.txt (56665 bytes)\n",
      "Saved: John_Lennon.txt (190580 bytes)\n",
      "Saved: John_Mayall.txt (40882 bytes)\n",
      "Saved: John_Mayall_&_the_Bluesbreakers.txt (12643 bytes)\n",
      "Saved: John_Mayer.txt (177873 bytes)\n",
      "Saved: John_Mellencamp.txt (95863 bytes)\n",
      "Saved: Journey_(band).txt (86193 bytes)\n",
      "Saved: Joy_Division.txt (56314 bytes)\n",
      "Saved: Judas_Priest.txt (142588 bytes)\n",
      "Saved: Kaiser_Chiefs.txt (45055 bytes)\n",
      "Saved: Kaleo_(band).txt (41897 bytes)\n",
      "Saved: Kansas_(band).txt (48947 bytes)\n",
      "Saved: Keane_(band).txt (61322 bytes)\n",
      "Saved: Kenny_Loggins.txt (46935 bytes)\n",
      "Saved: Kid_Rock.txt (113291 bytes)\n",
      "Saved: Killswitch_Engage.txt (97013 bytes)\n",
      "Saved: Kings_of_Leon.txt (47846 bytes)\n",
      "Saved: Kiss_(band).txt (171003 bytes)\n",
      "Saved: Korn.txt (185415 bytes)\n",
      "Saved: Lacuna_Coil.txt (45577 bytes)\n",
      "Saved: Lamb_of_God_(band).txt (103204 bytes)\n",
      "Saved: Led_Zeppelin.txt (137345 bytes)\n",
      "Saved: Lenny_Kravitz.txt (84860 bytes)\n",
      "Saved: Lifehouse_(band).txt (42995 bytes)\n",
      "Saved: Limp_Bizkit.txt (191987 bytes)\n",
      "Saved: Linda_Ronstadt.txt (190974 bytes)\n",
      "Saved: Linkin_Park.txt (242935 bytes)\n",
      "Saved: Little_Richard.txt (161799 bytes)\n",
      "Saved: Little_River_Band.txt (115997 bytes)\n",
      "Saved: Live_(band).txt (36154 bytes)\n",
      "Saved: Living_Colour.txt (36371 bytes)\n",
      "Saved: Lou_Reed.txt (103902 bytes)\n",
      "Saved: Loverboy.txt (23472 bytes)\n",
      "Saved: Lynyrd_Skynyrd.txt (60379 bytes)\n",
      "Saved: Manfred_Mann.txt (26613 bytes)\n",
      "Saved: Manfred_Mann_(musician).txt (15373 bytes)\n",
      "Saved: Manfred_Mann_Chapter_Three.txt (3832 bytes)\n",
      "Saved: Manfred_Mann's_Earth_Band.txt (42805 bytes)\n",
      "Saved: Marilyn_Manson.txt (233126 bytes)\n",
      "Saved: Marilyn_Manson_(band).txt (295622 bytes)\n",
      "Saved: Matchbox_Twenty.txt (37903 bytes)\n",
      "Saved: Meat_Loaf.txt (123473 bytes)\n",
      "Saved: Megadeth.txt (194114 bytes)\n",
      "Saved: Melissa_Etheridge.txt (58332 bytes)\n",
      "Saved: Men_at_Work.txt (45569 bytes)\n",
      "Saved: Metallica.txt (220439 bytes)\n",
      "Saved: Michael_Martin_Murphey.txt (40471 bytes)\n",
      "Saved: Midnight_Oil.txt (113705 bytes)\n",
      "Saved: Mike_and_the_Mechanics.txt (27007 bytes)\n",
      "Saved: Mitch_Ryder.txt (16202 bytes)\n",
      "Saved: Modest_Mouse.txt (42353 bytes)\n",
      "Saved: Montgomery_Gentry.txt (55109 bytes)\n",
      "Saved: Morrissey.txt (194789 bytes)\n",
      "Saved: Motörhead.txt (135310 bytes)\n",
      "Saved: Mudvayne.txt (51080 bytes)\n",
      "Saved: Mumford_&_Sons.txt (71532 bytes)\n",
      "Saved: Muse_(band).txt (100117 bytes)\n",
      "Saved: My_Chemical_Romance.txt (180895 bytes)\n",
      "Saved: Mötley_Crüe.txt (189760 bytes)\n",
      "Saved: Neil_Young.txt (181514 bytes)\n",
      "Saved: New_York_Dolls.txt (66486 bytes)\n",
      "Saved: Nickelback.txt (84657 bytes)\n",
      "Saved: Nine_Inch_Nails.txt (238098 bytes)\n",
      "Saved: Nirvana_(band).txt (98437 bytes)\n",
      "Saved: No_Doubt.txt (68453 bytes)\n",
      "Saved: Oasis_(band).txt (128698 bytes)\n",
      "Saved: Our_Lady_Peace.txt (61153 bytes)\n",
      "Saved: Ozzy_Osbourne.txt (212794 bytes)\n",
      "Saved: P.O.D.txt (48148 bytes)\n",
      "Saved: Panic!_at_the_Disco.txt (136768 bytes)\n",
      "Saved: Pantera.txt (105743 bytes)\n",
      "Saved: Papa_Roach.txt (106737 bytes)\n",
      "Saved: Paramore.txt (193298 bytes)\n",
      "Saved: Parliament_(band).txt (11829 bytes)\n",
      "Saved: Parliament_-Funkadelic.txt (0 bytes)\n",
      "Saved: Pat_Benatar.txt (58307 bytes)\n",
      "Saved: Paul_McCartney.txt (265771 bytes)\n",
      "Saved: Paul_McCartney_and_Wings.txt (69243 bytes)\n",
      "Saved: Pearl_Jam.txt (134545 bytes)\n",
      "Saved: Peter_Frampton.txt (60255 bytes)\n",
      "Saved: Peter_Gabriel.txt (128308 bytes)\n",
      "Saved: Phil_Collins.txt (188874 bytes)\n",
      "Saved: Pink_Floyd.txt (187572 bytes)\n",
      "Saved: Pixies_(band).txt (98420 bytes)\n",
      "Saved: Pixies_(band).txt (98420 bytes)\n",
      "Saved: Pop_Evil.txt (23333 bytes)\n",
      "Saved: Primus_(band).txt (90568 bytes)\n",
      "Saved: Puddle_of_Mudd.txt (54800 bytes)\n",
      "Saved: Queen_(band).txt (283945 bytes)\n",
      "Saved: Queens_of_the_Stone_Age.txt (88124 bytes)\n",
      "Saved: Queensrÿche.txt (81181 bytes)\n",
      "Saved: Quiet_Riot.txt (68949 bytes)\n",
      "Saved: R.E.M.txt (120342 bytes)\n",
      "Saved: REO_Speedwagon.txt (37256 bytes)\n",
      "Saved: Radiohead.txt (218414 bytes)\n",
      "Saved: Rage_Against_the_Machine.txt (140113 bytes)\n",
      "Saved: Rainbow_(rock_band).txt (37953 bytes)\n",
      "Saved: Rammstein.txt (107174 bytes)\n",
      "Saved: Ramones.txt (118169 bytes)\n",
      "Saved: Ratt.txt (47218 bytes)\n",
      "Saved: Red_Hot_Chili_Peppers.txt (154652 bytes)\n",
      "Saved: Ringo_Starr.txt (149149 bytes)\n",
      "Saved: Rise_Against.txt (113444 bytes)\n",
      "Saved: Ritchie_Valens.txt (40631 bytes)\n",
      "Saved: Rob_Zombie.txt (129077 bytes)\n",
      "Saved: Robert_Plant.txt (92391 bytes)\n",
      "Saved: Rod_Stewart.txt (133053 bytes)\n",
      "Saved: Roxy_Music.txt (61122 bytes)\n",
      "Saved: Roy_Orbison.txt (100919 bytes)\n",
      "Saved: Royal_Blood_(band).txt (33887 bytes)\n",
      "Saved: Run-DMC.txt (54767 bytes)\n",
      "Saved: Rush_(band).txt (169552 bytes)\n",
      "Saved: Saliva_(band).txt (40938 bytes)\n",
      "Saved: Sam_Fender.txt (176508 bytes)\n",
      "Saved: Santana_(band).txt (48914 bytes)\n",
      "Saved: Saving_Abel.txt (15156 bytes)\n",
      "Saved: Scorpions_(band).txt (69196 bytes)\n",
      "Saved: Seether.txt (46546 bytes)\n",
      "Saved: Sepultura.txt (97317 bytes)\n",
      "Saved: Sex_Pistols.txt (116064 bytes)\n",
      "Saved: Shakin'_Stevens.txt (26533 bytes)\n",
      "Saved: Sheryl_Crow.txt (88535 bytes)\n",
      "Saved: Shinedown.txt (81921 bytes)\n",
      "Saved: Silverchair.txt (100412 bytes)\n",
      "Saved: Simon_&_Garfunkel.txt (83262 bytes)\n",
      "Saved: Simple_Minds.txt (108473 bytes)\n",
      "Saved: Simple_Plan.txt (63222 bytes)\n",
      "Saved: Sixto_Rodriguez.txt (49596 bytes)\n",
      "Saved: Skid_Row_(American_band).txt (53279 bytes)\n",
      "Saved: Skillet_(band).txt (78544 bytes)\n",
      "Saved: Slade.txt (126893 bytes)\n",
      "Saved: Slayer.txt (166057 bytes)\n",
      "Saved: Slipknot_(band).txt (149358 bytes)\n",
      "Saved: Sly_and_the_Family_Stone.txt (58444 bytes)\n",
      "Saved: Small_Faces.txt (50561 bytes)\n",
      "Saved: Smash_Mouth.txt (57658 bytes)\n",
      "Saved: Smokie_(band).txt (29809 bytes)\n",
      "Saved: Snow_Patrol.txt (67050 bytes)\n",
      "Saved: Social_Distortion.txt (56789 bytes)\n",
      "Saved: Sonic_Youth.txt (78169 bytes)\n",
      "Saved: Soundgarden.txt (133574 bytes)\n",
      "Saved: Staind.txt (47546 bytes)\n",
      "Saved: Starset.txt (52699 bytes)\n",
      "Saved: Starship_(band).txt (37306 bytes)\n",
      "Saved: Status_Quo_(band).txt (64473 bytes)\n",
      "Saved: Steely_Dan.txt (65364 bytes)\n",
      "Saved: Steppenwolf_(band).txt (24755 bytes)\n",
      "Saved: Steve_Miller_Band.txt (27657 bytes)\n",
      "Saved: Steve_Vai.txt (111576 bytes)\n",
      "Saved: Steve_Winwood.txt (54930 bytes)\n",
      "Saved: Stevie_Nicks.txt (144413 bytes)\n",
      "Saved: Stevie_Ray_Vaughan.txt (102585 bytes)\n",
      "Saved: Sting_(musician).txt (115907 bytes)\n",
      "Saved: Stone_Sour.txt (43107 bytes)\n",
      "Saved: Stone_Temple_Pilots.txt (66029 bytes)\n",
      "Saved: Styx_(band).txt (55441 bytes)\n",
      "Saved: Sublime_(band).txt (48326 bytes)\n",
      "Saved: Sublime_with_Rome.txt (36931 bytes)\n",
      "Saved: Sum_41.txt (203191 bytes)\n",
      "Saved: Supertramp.txt (50763 bytes)\n",
      "Saved: Survivor_(band).txt (45597 bytes)\n",
      "Saved: System_of_a_Down.txt (115885 bytes)\n",
      "Saved: T._Rex_(band).txt (63924 bytes)\n",
      "Saved: Talking_Heads.txt (55577 bytes)\n",
      "Saved: Ted_Nugent.txt (112217 bytes)\n",
      "Saved: Ten_Years_After.txt (18259 bytes)\n",
      "Saved: Tenacious_D.txt (116064 bytes)\n",
      "Saved: Tesla_(band).txt (29925 bytes)\n",
      "Saved: The_Alan_Parsons_Project.txt (23001 bytes)\n",
      "Saved: The_All_-American_Rejects.txt (0 bytes)\n",
      "Saved: The_Allman_Brothers_Band.txt (81348 bytes)\n",
      "Saved: The_Animals.txt (30600 bytes)\n",
      "Saved: The_All_-American_Rejects.txt (0 bytes)\n",
      "Saved: The_Band.txt (81032 bytes)\n",
      "Saved: The_Bangles.txt (35702 bytes)\n",
      "Saved: The_Beach_Boys.txt (233535 bytes)\n",
      "Saved: The_Beatles.txt (226718 bytes)\n",
      "Saved: The_Big_Bopper.txt (31170 bytes)\n",
      "Saved: The_Black_Crowes.txt (39041 bytes)\n",
      "Saved: The_Black_Keys.txt (111961 bytes)\n",
      "Saved: The_Boxtones.txt (15318 bytes)\n",
      "Saved: The_Byrds.txt (178936 bytes)\n",
      "Saved: The_Cab.txt (53298 bytes)\n",
      "Saved: The_Cardigans.txt (26500 bytes)\n",
      "Saved: The_Cars.txt (26574 bytes)\n",
      "Saved: The_Clash.txt (95628 bytes)\n",
      "Saved: The_Cranberries.txt (124596 bytes)\n",
      "Saved: The_Cult.txt (91196 bytes)\n",
      "Saved: The_Cure.txt (102995 bytes)\n",
      "Saved: The_Dave_Clark_Five.txt (24815 bytes)\n",
      "Saved: The_Doobie_Brothers.txt (71808 bytes)\n",
      "Saved: The_Doors.txt (99955 bytes)\n",
      "Saved: The_Everly_Brothers.txt (56116 bytes)\n",
      "Saved: The_Fixx.txt (11763 bytes)\n",
      "Saved: The_Flaming_Lips.txt (72552 bytes)\n",
      "Saved: The_Four_Seasons_(band).txt (91119 bytes)\n",
      "Saved: The_Fray.txt (55903 bytes)\n",
      "Saved: The_Go_-Go's.txt (0 bytes)\n",
      "Saved: The_Guess_Who.txt (48512 bytes)\n",
      "Saved: The_Hives.txt (39866 bytes)\n",
      "Saved: The_Hollies.txt (39711 bytes)\n",
      "Saved: The_J._Geils_Band.txt (31188 bytes)\n",
      "Saved: The_Jam.txt (52811 bytes)\n",
      "Saved: The_Jesus_and_Mary_Chain.txt (57457 bytes)\n",
      "Saved: The_Killers.txt (126790 bytes)\n",
      "Saved: The_Kinks.txt (122393 bytes)\n",
      "Saved: The_Lovin'_Spoonful.txt (167467 bytes)\n",
      "Saved: The_Lumineers.txt (61393 bytes)\n",
      "Saved: The_Mamas_&_the_Papas.txt (51496 bytes)\n",
      "Saved: The_Marshall_Tucker_Band.txt (49045 bytes)\n",
      "Saved: The_Monkees.txt (110221 bytes)\n",
      "Saved: The_Moody_Blues.txt (86922 bytes)\n",
      "Saved: The_Mothers_of_Invention.txt (56222 bytes)\n",
      "Saved: The_Offspring.txt (111939 bytes)\n",
      "Saved: The_Outfield.txt (39565 bytes)\n",
      "Saved: The_Police.txt (80360 bytes)\n",
      "Saved: The_Presidents_of_the_United_States_of_America_(band).txt (26985 bytes)\n",
      "Saved: The_Pretenders.txt (29209 bytes)\n",
      "Saved: The_Pretty_Reckless.txt (88964 bytes)\n",
      "Saved: The_Rolling_Stones.txt (269216 bytes)\n",
      "Saved: The_Script.txt (43119 bytes)\n",
      "Saved: The_Smashing_Pumpkins.txt (140217 bytes)\n",
      "Saved: The_Smiths.txt (97401 bytes)\n",
      "Saved: The_Stone_Roses.txt (62613 bytes)\n",
      "Saved: The_Strokes.txt (66407 bytes)\n",
      "Saved: The_Sweet.txt (64420 bytes)\n",
      "Saved: The_Tragically_Hip.txt (65075 bytes)\n",
      "Saved: The_Used.txt (52490 bytes)\n",
      "Saved: The_Velvet_Underground.txt (94361 bytes)\n",
      "Saved: The_Verve.txt (46220 bytes)\n",
      "Saved: The_White_Stripes.txt (176956 bytes)\n",
      "Saved: The_Who.txt (179798 bytes)\n",
      "Saved: The_Yardbirds.txt (57597 bytes)\n",
      "Saved: The_Zombies.txt (58697 bytes)\n",
      "Saved: Theory_of_a_Deadman.txt (39636 bytes)\n",
      "Saved: Thin_Lizzy.txt (87643 bytes)\n",
      "Saved: Third_Eye_Blind.txt (86291 bytes)\n",
      "Saved: Thirty_Seconds_to_Mars.txt (66981 bytes)\n",
      "Saved: Thousand_Foot_Krutch.txt (45689 bytes)\n",
      "Saved: Three_Days_Grace.txt (126739 bytes)\n",
      "Saved: Three_Dog_Night.txt (38533 bytes)\n",
      "Saved: Tom_Petty.txt (95073 bytes)\n",
      "Saved: Tom_Petty_and_the_Heartbreakers.txt (42177 bytes)\n",
      "Saved: Tommy_James_and_the_Shondells.txt (26176 bytes)\n",
      "Saved: Tool_(band).txt (138306 bytes)\n",
      "Saved: Toto_(band).txt (70183 bytes)\n",
      "Saved: Tracy_Chapman.txt (49753 bytes)\n",
      "Saved: Traffic_(band).txt (30934 bytes)\n",
      "Saved: Train_(band).txt (75427 bytes)\n",
      "Saved: Traveling_Wilburys.txt (63434 bytes)\n",
      "Saved: Travis_(band).txt (45708 bytes)\n",
      "Saved: Trivium_(band).txt (92173 bytes)\n",
      "Saved: Twenty_One_Pilots.txt (198454 bytes)\n",
      "Saved: Twisted_Sister.txt (44931 bytes)\n",
      "Saved: U2.txt (263381 bytes)\n",
      "Saved: UB40.txt (68179 bytes)\n",
      "Saved: Uriah_Heep_(band).txt (67107 bytes)\n",
      "Saved: Vampire_Weekend.txt (65996 bytes)\n",
      "Saved: Van_Halen.txt (107535 bytes)\n",
      "Saved: Van_Morrison.txt (201496 bytes)\n",
      "Saved: Tommy_James_and_the_Shondells.txt (26176 bytes)\n",
      "Saved: Velvet_Revolver.txt (68154 bytes)\n",
      "Saved: Volbeat.txt (40420 bytes)\n",
      "Saved: W.A.S.P._(band).txt (38690 bytes)\n",
      "Saved: Warrant_(American_band).txt (31480 bytes)\n",
      "Saved: Weezer.txt (169304 bytes)\n",
      "Saved: White_Zombie_(band).txt (28836 bytes)\n",
      "Saved: Whitesnake.txt (175179 bytes)\n",
      "Saved: X_(American_band).txt (32895 bytes)\n",
      "Saved: X_Ambassadors.txt (36635 bytes)\n",
      "Saved: Yes_(band).txt (133757 bytes)\n",
      "Saved: ZZ_Top.txt (64775 bytes)\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, pathlib, requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "INPUT_LIST = \"rock_artists.txt\"      # one title per line\n",
    "OUT_DIR     = \"pages_raw_wikitext\"   # output folder\n",
    "SLEEP_SEC   = 0.2                    # gentle rate limit\n",
    "\n",
    "API = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# MediaWiki lets up to ~50 titles per request for 'query'\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "def safe_filename(name: str, maxlen: int = 150) -> str:\n",
    "    \"\"\"\n",
    "    Make a Windows-safe filename from a page title.\n",
    "    Replaces / \\ : * ? \" < > | with underscores, trims spaces/dots.\n",
    "    \"\"\"\n",
    "    name = re.sub(r'[\\\\/:*?\"<>|]+', \"_\", name)\n",
    "    name = name.strip(\" .\")\n",
    "    if len(name) > maxlen:\n",
    "        name = name[:maxlen]\n",
    "    return name\n",
    "\n",
    "def to_api_title(title):\n",
    "    return title.strip().replace(\" \", \"_\")\n",
    "\n",
    "def load_titles(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        titles = [line.strip() for line in f if line.strip()]\n",
    "    # Deduplicate but preserve order\n",
    "    seen, clean = set(), []\n",
    "    for t in titles:\n",
    "        norm = to_api_title(t)\n",
    "        if norm not in seen:\n",
    "            clean.append(norm)\n",
    "            seen.add(norm)\n",
    "    return clean\n",
    "\n",
    "def batched(iterable, n):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]\n",
    "\n",
    "def fetch_wikitext_batch(titles):\n",
    "    \"\"\"\n",
    "    Fetch wikitext via action=query&prop=revisions&rvslots=main&rvprop=content\n",
    "    Handles redirects. Returns dict: {normalized_title: {\"title\":..., \"pageid\":..., \"wikitext\":...}}\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"redirects\": 1,\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"formatversion\": 2,\n",
    "        \"titles\": \"|\".join(titles),\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"RockNetworkBot/0.1 (https://github.com/yourusername; your_email@example.com)\"\n",
    "    }\n",
    "    r = requests.get(API, params=params, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    ...\n",
    "\n",
    "\n",
    "    # Build normalization/redirect mapping from response (if present)\n",
    "    title_map = {t: t for t in titles}\n",
    "    for norm in data.get(\"query\", {}).get(\"normalized\", []) or []:\n",
    "        title_map[norm[\"from\"].replace(\" \", \"_\")] = norm[\"to\"].replace(\" \", \"_\")\n",
    "    for redir in data.get(\"query\", {}).get(\"redirects\", []) or []:\n",
    "        title_map[redir[\"from\"].replace(\" \", \"_\")] = redir[\"to\"].replace(\" \", \"_\")\n",
    "\n",
    "    out = {}\n",
    "    for page in data.get(\"query\", {}).get(\"pages\", []):\n",
    "        title = page.get(\"title\", \"\").replace(\" \", \"_\")\n",
    "        pageid = page.get(\"pageid\", None)\n",
    "        revs = page.get(\"revisions\", [])\n",
    "        wikitext = \"\"\n",
    "        if revs and \"slots\" in revs[0] and \"main\" in revs[0][\"slots\"]:\n",
    "            wikitext = revs[0][\"slots\"][\"main\"].get(\"content\", \"\")\n",
    "\n",
    "        out[title] = {\n",
    "            \"title\": title,\n",
    "            \"pageid\": pageid,\n",
    "            \"wikitext\": wikitext,\n",
    "            \"missing\": page.get(\"missing\", False)\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    titles = load_titles(INPUT_LIST)\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    for chunk in batched(titles, BATCH_SIZE):\n",
    "        try:\n",
    "            data = fetch_wikitext_batch(chunk)\n",
    "        except Exception as e:\n",
    "            print(\"Batch failed:\", e)\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "\n",
    "        for title in chunk:\n",
    "            # Resolve final title if API redirected/normalized it\n",
    "            # We’ll choose the returned key if present\n",
    "            # Fall back to the original requested title\n",
    "            saved = None\n",
    "            # Prefer exact returned match if found\n",
    "            for k in (title,) + tuple(data.keys()):\n",
    "                if k in data:\n",
    "                    saved = data[k]\n",
    "                    break\n",
    "            if not saved:\n",
    "                # Nothing came back—skip\n",
    "                print(f\"[WARN] No data for {title}\")\n",
    "                continue\n",
    "\n",
    "            final_title = saved[\"title\"] or title\n",
    "            safe_title  = safe_filename(final_title)   # sanitize it\n",
    "            fname = f\"{safe_title}.txt\"\n",
    "            fpath = pathlib.Path(OUT_DIR) / fname\n",
    "\n",
    "            # Save raw wikitext; also drop a tiny JSON sidecar with metadata\n",
    "            with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(saved.get(\"wikitext\", \"\"))\n",
    "\n",
    "            meta = {\n",
    "                \"requested_title\": title,\n",
    "                \"final_title\": final_title,\n",
    "                \"pageid\": saved.get(\"pageid\"),\n",
    "                \"missing\": saved.get(\"missing\", False),\n",
    "                \"bytes\": len(saved.get(\"wikitext\", \"\")),\n",
    "            }\n",
    "            with open(str(fpath) + \".json\", \"w\", encoding=\"utf-8\") as jf:\n",
    "                json.dump(meta, jf, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"Saved: {fname} ({meta['bytes']} bytes)\")\n",
    "        time.sleep(SLEEP_SEC)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0770c403",
   "metadata": {},
   "source": [
    "### *Step 3: Build Rock Network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844690d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolates: ['Van_Zant_(band)', 'Dr._Hook_&_the_Medicine_Show', 'Jet_(Australian_band)']\n",
      "Nodes: 486\n",
      "Edges: 8378\n",
      "AC/DC in-degree: 41\n",
      "AC/DC out-degree: 34\n",
      "Black_Sabbath in-degree: 74\n",
      "Black_Sabbath out-degree: 64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "import networkx as nx\n",
    "\n",
    "PAGES_DIR = pathlib.Path(\"pages_raw_wikitext\")\n",
    "ARTISTS_TXT = \"rock_artists.txt\"\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "# Capture [[Target]] and [[Target|display]], ignoring section anchors (after #)\n",
    "LINK_RE = re.compile(r\"\\[\\[([^\\[\\]\\|#]+)(?:\\|[^\\[\\]]+)?\\]\\]\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ']+\")\n",
    "\n",
    "# NEW/CHANGED: normalization regexes\n",
    "DASH_RE = re.compile(r\"\\s*[\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015-]\\s*\")  # all common dash chars\n",
    "APOST_RE = re.compile(r\"[’‘`´]\")  # curly/smart apostrophes & accents -> '\n",
    "AND_TOKEN_RE = re.compile(r\"\\s+(?:&|and)\\s+\", flags=re.IGNORECASE)  # unify &/and during variants\n",
    "\n",
    "def normalize_basic(s: str) -> str:\n",
    "    \"\"\"Normalize punctuation and spacing so variant lookups become reliable.\"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)   # collapse whitespace\n",
    "    s = APOST_RE.sub(\"'\", s)     # unify apostrophes\n",
    "    s = DASH_RE.sub(\"-\", s)      # unify dashes and trim spaces around them\n",
    "    return s\n",
    "\n",
    "def norm_key(s: str) -> str:\n",
    "    \"\"\"Key used for the variant index: normalized + lowercased.\"\"\"\n",
    "    return normalize_basic(s).lower() if s else s\n",
    "\n",
    "def extract_links_from_wikitext(text: str):\n",
    "    \"\"\"Return list of raw wiki targets e.g. 'Black Sabbath' or 'AC/DC' (spaces preserved here).\"\"\"\n",
    "    return [m.group(1).strip() for m in LINK_RE.finditer(text)]\n",
    "\n",
    "def canon_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Canonical node label:\n",
    "    - Normalize punctuation/spacing\n",
    "    - Replace spaces with underscores\n",
    "    - KEEP slashes (e.g., AC/DC stays AC/DC)\n",
    "    \"\"\"\n",
    "    t = normalize_basic(title)  # NEW/CHANGED\n",
    "    return t.replace(\"  \", \" \").replace(\" \", \"_\")\n",
    "\n",
    "# NEW/CHANGED: helper to generate &/and forms\n",
    "def amp_variants(s: str):\n",
    "    base = normalize_basic(s)\n",
    "    with_amp = AND_TOKEN_RE.sub(\" & \", base)\n",
    "    with_and = AND_TOKEN_RE.sub(\" and \", base)\n",
    "    return {base, with_amp, with_and}\n",
    "\n",
    "def variants_for_lookup(title: str):\n",
    "    \"\"\"\n",
    "    Variants to help map filenames/links to canonical performer titles.\n",
    "    We include: spaces, underscores, slashes, &/and forms, and lowercase.\n",
    "    (We still store them via norm_key, so case differences are collapsed.)\n",
    "    \"\"\"\n",
    "    variants = set()\n",
    "    for seed in amp_variants(title):\n",
    "        # start from normalized seed\n",
    "        seed = normalize_basic(seed)\n",
    "        t_space = seed.replace(\"_\", \" \")\n",
    "        t_under = seed.replace(\" \", \"_\")\n",
    "        # swap in slashes from both space and underscore forms\n",
    "        t_slash_from_under = t_under.replace(\"_\", \"/\")\n",
    "        t_slash_from_space = t_space.replace(\" \", \"/\")\n",
    "\n",
    "        variants.update({\n",
    "            seed, t_space, t_under, t_slash_from_under, t_slash_from_space,\n",
    "            seed.lower(), t_space.lower(), t_under.lower(),\n",
    "            t_slash_from_under.lower(), t_slash_from_space.lower(),\n",
    "        })\n",
    "\n",
    "        # also include minor hyphen/space variants (after dash normalization this is small)\n",
    "        variants.add(seed.replace(\"-\", \" - \"))\n",
    "        variants.add(seed.replace(\" - \", \"-\"))\n",
    "\n",
    "    # final pass: normalize again so weird combos collapse\n",
    "    return {normalize_basic(v) for v in variants}\n",
    "\n",
    "def build_performer_index(performers_set):\n",
    "    \"\"\"\n",
    "    Build a mapping from many variants -> canonical performer title (canonical uses underscores, keeps slashes).\n",
    "    Uses norm_key() so lookups are consistent.\n",
    "    \"\"\"\n",
    "    idx = {}\n",
    "    for p in performers_set:\n",
    "        for v in variants_for_lookup(p):\n",
    "            idx[norm_key(v)] = p  # NEW/CHANGED\n",
    "    return idx\n",
    "\n",
    "def count_words(text: str) -> int:\n",
    "    return len(WORD_RE.findall(text))\n",
    "\n",
    "# -------------------------\n",
    "# Load performers\n",
    "# -------------------------\n",
    "with open(ARTISTS_TXT, encoding=\"utf-8\") as f:\n",
    "    # Canonicalize with normalization: replace spaces with underscores; keep slashes in names like AC/DC\n",
    "    performers = {canon_title(line) for line in f if line.strip()}  # canon_title() now normalizes\n",
    "\n",
    "# Build a variant index for robust matching\n",
    "index = build_performer_index(performers)\n",
    "\n",
    "# -------------------------\n",
    "# Build the directed graph\n",
    "# -------------------------\n",
    "G = nx.DiGraph()\n",
    "for p in performers:\n",
    "    # initialize nodes with a placeholder word count\n",
    "    G.add_node(p, words=0, title=p.replace(\"_\", \" \"))  # store a human-readable title as well\n",
    "\n",
    "# Iterate saved pages (*.txt)\n",
    "for txt_path in PAGES_DIR.glob(\"*.txt\"):\n",
    "    # 1) Determine the true wiki title for this file (prefer sidecar .json if exists)\n",
    "    meta_path = txt_path.with_suffix(\".txt.json\")\n",
    "    if meta_path.exists():\n",
    "        meta = json.loads(meta_path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "        final_title = meta.get(\"final_title\") or txt_path.stem\n",
    "    else:\n",
    "        final_title = txt_path.stem\n",
    "\n",
    "    # Canonicalize the source node label (normalize + spaces->underscores, keep slashes)\n",
    "    src_canon = canon_title(final_title)\n",
    "\n",
    "    # If not directly known, try variant mapping using filename stem as well\n",
    "    if src_canon not in performers:\n",
    "        # try a few variants from stem and final_title to find a performer\n",
    "        candidates = set()\n",
    "        for raw in {txt_path.stem, final_title, src_canon}:\n",
    "            candidates |= variants_for_lookup(raw)\n",
    "            candidates |= variants_for_lookup(canon_title(raw))\n",
    "        # NEW/CHANGED: index lookup via norm_key\n",
    "        src_mapped = next((index.get(norm_key(v)) for v in candidates if norm_key(v) in index), None)\n",
    "        if not src_mapped:\n",
    "            # couldn't map this file to a performer in your list\n",
    "            continue\n",
    "        src = src_mapped\n",
    "    else:\n",
    "        src = src_canon\n",
    "\n",
    "    # 2) Read wikitext and set node word count\n",
    "    text = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    G.nodes[src][\"words\"] = count_words(text)\n",
    "\n",
    "    # 3) Extract outgoing link targets from wikitext\n",
    "    raw_links = extract_links_from_wikitext(text)\n",
    "\n",
    "    # 4) Normalize and add edges only if target is another performer\n",
    "    for tgt_raw in raw_links:\n",
    "        # Prefer canonicalization path first\n",
    "        tgt_canon_guess = canon_title(tgt_raw)  # canon_title() includes normalization now\n",
    "\n",
    "        if tgt_canon_guess in performers:\n",
    "            tgt = tgt_canon_guess\n",
    "        else:\n",
    "            # NEW/CHANGED: single consistent lookup key\n",
    "            tgt = index.get(norm_key(tgt_raw))\n",
    "\n",
    "        if tgt and tgt != src and tgt in performers:\n",
    "            G.add_edge(src, tgt)\n",
    "\n",
    "# -------------------------\n",
    "\n",
    "# Prune isolates and keep largest weakly connected component\n",
    "# -------------------------\n",
    "isolates = list(nx.isolates(G))\n",
    "print(\"Isolates:\", isolates)\n",
    "G.remove_nodes_from(isolates)\n",
    "\n",
    "if G.number_of_nodes() > 0:\n",
    "    lcc_nodes = max(nx.weakly_connected_components(G), key=len)\n",
    "    G_lcc = G.subgraph(lcc_nodes).copy()\n",
    "else:\n",
    "    G_lcc = G\n",
    "\n",
    "# -------------------------\n",
    "# Inspect and save\n",
    "# -------------------------\n",
    "print(\"Nodes:\", G_lcc.number_of_nodes())\n",
    "print(\"Edges:\", G_lcc.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe0d44",
   "metadata": {},
   "source": [
    "### *Step 4: Generate GEXF, GraphML and json files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3891f164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Malin Haugland Høli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\readwrite\\json_graph\\node_link.py:145: FutureWarning: \n",
      "The default value will be `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "\n",
    "# GEXF (good for Gephi, Cytoscape, and reloading into NetworkX)\n",
    "nx.write_gexf(G_lcc, \"rock_network.gexf\")\n",
    "\n",
    "# GraphML (also widely supported)\n",
    "nx.write_graphml(G_lcc, \"rock_network.graphml\")\n",
    "\n",
    "# Or JSON (node-link format)\n",
    "with open(\"rock_network.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_graph.node_link_data(G_lcc), f, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
