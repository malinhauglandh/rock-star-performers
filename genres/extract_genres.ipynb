{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce70c37e",
   "metadata": {},
   "source": [
    "### Fetch wikitext and extract the genres for all rock artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af216774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved genres.json\n"
     ]
    }
   ],
   "source": [
    "# End-to-end: fetch wikitext -> parse infobox genres -> normalize -> save genres.json\n",
    "\n",
    "from tracemalloc import start\n",
    "from matplotlib import lines\n",
    "import re, html, json, time, unicodedata, requests\n",
    "from typing import List, Optional\n",
    "\n",
    "API = \"https://en.wikipedia.org/w/api.php\"\n",
    "HEADERS = {\"User-Agent\": \"GenreExtractor/1.0 (student project; email@example.com)\"}\n",
    "\n",
    "# ---------- Fetch (resilient) ----------\n",
    "def canonicalize_title(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFC\", s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"\\s*[-–—]\\s*\", \"–\", s)  # normalize hyphen/en/em-dash to en dash, no spaces\n",
    "    return s.replace(\" \", \"_\")\n",
    "\n",
    "def search_best_title(query: str) -> Optional[str]:\n",
    "    q = query.replace(\"_\", \" \")\n",
    "    try:\n",
    "        r = requests.get(API, params={\n",
    "            \"action\":\"opensearch\", \"search\": q, \"limit\": 1, \"namespace\": 0, \"format\": \"json\"\n",
    "        }, headers=HEADERS, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if len(data) > 1 and data[1]:\n",
    "            return data[1][0].replace(\" \", \"_\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def fetch_via_query(title: str) -> tuple[str, bool]:\n",
    "    try:\n",
    "        r = requests.get(API, params={\n",
    "            \"action\":\"query\",\"format\":\"json\",\"redirects\":1,\"converttitles\":1,\n",
    "            \"prop\":\"revisions\",\"rvslots\":\"main\",\"rvprop\":\"content\",\"formatversion\":2,\n",
    "            \"titles\": title,\n",
    "        }, headers=HEADERS, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", [])\n",
    "        if not pages: return \"\", True\n",
    "        p = pages[0]\n",
    "        revs = p.get(\"revisions\", [])\n",
    "        if revs:\n",
    "            wikitext = revs[0].get(\"slots\", {}).get(\"main\", {}).get(\"content\", \"\") or \"\"\n",
    "        else:\n",
    "            wikitext = \"\"\n",
    "        return wikitext, bool(p.get(\"missing\"))\n",
    "    except Exception:\n",
    "        return \"\", True\n",
    "\n",
    "def fetch_via_parse(title: str) -> tuple[str, bool]:\n",
    "    try:\n",
    "        r = requests.get(API, params={\n",
    "            \"action\":\"parse\",\"page\":title,\"prop\":\"wikitext\",\"format\":\"json\",\n",
    "            \"formatversion\":2,\"redirects\":1,\"converttitles\":1,\n",
    "        }, headers=HEADERS, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if \"error\" in data: return \"\", True\n",
    "        wikitext = data.get(\"parse\", {}).get(\"wikitext\", \"\") or \"\"\n",
    "        return wikitext, not bool(wikitext)\n",
    "    except Exception:\n",
    "        return \"\", True\n",
    "\n",
    "def fetch_wikitext_resilient(raw_title: str, sleep_between=0.2) -> tuple[str, str]:\n",
    "    t0 = canonicalize_title(raw_title)\n",
    "    w, miss = fetch_via_query(t0)\n",
    "    if w: return t0, w\n",
    "    time.sleep(sleep_between)\n",
    "    w, miss = fetch_via_parse(t0)\n",
    "    if w: return t0, w\n",
    "    best = search_best_title(t0)\n",
    "    if not best: return t0, \"\"\n",
    "    time.sleep(sleep_between)\n",
    "    w, miss = fetch_via_query(best)\n",
    "    if w: return best, w\n",
    "    time.sleep(sleep_between)\n",
    "    w, miss = fetch_via_parse(best)\n",
    "    return best, (w or \"\")\n",
    "\n",
    "# ---------- Parse infobox -> genre field ----------\n",
    "def extract_infobox(wikitext: str) -> Optional[str]:\n",
    "    name_re = re.compile(r\"^\\s*\\{\\{\\s*infobox\\s+([^\\n{|}]*)\", flags=re.I)\n",
    "    preferred = re.compile(r\"\\b(musical|music|artist|singer|band|group)\\b\", flags=re.I)\n",
    "\n",
    "    i, n = 0, len(wikitext)\n",
    "    best = None\n",
    "\n",
    "    while i < n:\n",
    "        m = re.search(r\"\\{\\{[Ii]nfobox\", wikitext[i:])\n",
    "        if not m:\n",
    "            break\n",
    "        start = i + m.start()\n",
    "\n",
    "        # Find matching \"}}\" using brace depth\n",
    "        depth, j = 0, start\n",
    "        while j < n:\n",
    "            if wikitext.startswith(\"{{\", j):\n",
    "                depth += 1; j += 2\n",
    "                continue\n",
    "            if wikitext.startswith(\"}}\", j):\n",
    "                depth -= 1; j += 2\n",
    "                if depth == 0:\n",
    "                    end = j\n",
    "                    box = wikitext[start:end]\n",
    "\n",
    "                    # Read the infobox name to rank it\n",
    "                    name_line = wikitext[start:start+200].split(\"\\n\", 1)[0]\n",
    "                    nm = name_re.search(name_line)\n",
    "                    name = nm.group(1).strip().lower() if nm else \"\"\n",
    "\n",
    "                    # Prefer ‘musical’ flavored infoboxes\n",
    "                    if preferred.search(name):\n",
    "                        return box\n",
    "                    if best is None:\n",
    "                        best = box\n",
    "                    break\n",
    "            j += 1\n",
    "        i = start + 2\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def extract_field_from_infobox(infobox: str, field_names: List[str]) -> Optional[str]:\n",
    "    # Accept genre, genres, and genre(s)\n",
    "    field_pat = re.compile(r\"^\\s*\\|\\s*genre(?:s|\\(s\\))?\\s*=\", flags=re.IGNORECASE)\n",
    "\n",
    "    # NEW: split into lines but strip any leading HTML comments before checking\n",
    "    lines = infobox.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").split(\"\\n\")\n",
    "    start = None\n",
    "    for idx, raw_line in enumerate(lines):\n",
    "        line = raw_line\n",
    "        # Strip any number of leading comments on this line (e.g., `<!-- ... -->| genre =`)\n",
    "        # Do it repeatedly in case there are multiple\n",
    "        while True:\n",
    "            new_line = re.sub(r\"^\\s*<!--.*?-->\\s*\", \"\", line)\n",
    "            if new_line == line:\n",
    "                break\n",
    "            line = new_line\n",
    "        if field_pat.match(line):\n",
    "            start = idx\n",
    "            # keep only the value after '=' from the cleaned line\n",
    "            value_after_eq = line.split(\"=\", 1)[1]\n",
    "            lines[idx] = value_after_eq  # replace just for buffer assembly\n",
    "            break\n",
    "\n",
    "    if start is None:\n",
    "        return None\n",
    "\n",
    "    buf = [lines[start].lstrip()]\n",
    "\n",
    "    # NEW: unconditional stop at the next real field (prevents bleed from e.g., instruments=)\n",
    "    next_field = re.compile(r\"^\\s*\\|\\s*[^|=}{\\n]+\\s*=\", flags=re.IGNORECASE)\n",
    "\n",
    "    for j in range(start + 1, len(lines)):\n",
    "        line = lines[j]\n",
    "        if next_field.match(line):\n",
    "            break\n",
    "        buf.append(line)\n",
    "\n",
    "    return \"\\n\".join(buf).strip()\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Cleanup & normalization ----------\n",
    "def _unwrap_list_templates(t: str) -> str:\n",
    "    # unwrap simple wrappers inside (nowrap/nobr/small/span)\n",
    "    for wrapper in (\"nowrap\",\"nobr\",\"small\",\"span\"):\n",
    "        t = re.sub(rf\"\\{{\\{{\\s*{wrapper}\\s*\\|(.+?)\\}}\\}}\", r\"\\1\",\n",
    "                   t, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # hlist/ubl/unbulleted list  -> join positional items with ';'\n",
    "    pat_pipe = re.compile(\n",
    "        r\"\\{\\{\\s*(h\\s*list|ubl|unbulleted\\s*list)\\s*\\|(.+?)\\}\\}\",\n",
    "        flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    def repl_pipe(m):\n",
    "        content = m.group(2)\n",
    "        # keep only positional args (no '=')\n",
    "        parts = [p.strip() for p in content.split(\"|\") if p.strip() and \"=\" not in p]\n",
    "        return \";\".join(parts)\n",
    "\n",
    "    # flatlist/plainlist -> take bullet lines or positional args\n",
    "    pat_bul = re.compile(\n",
    "        r\"\\{\\{\\s*(flat\\s*list|plain\\s*list)\\s*\\|(.+?)\\}\\}\",\n",
    "        flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    def repl_bul(m):\n",
    "        content = m.group(2)\n",
    "        items = []\n",
    "        for ln in content.splitlines():\n",
    "            ln = ln.strip()\n",
    "            if ln.startswith(\"*\"):\n",
    "                items.append(re.sub(r\"^\\*\\s*\", \"\", ln))\n",
    "        if not items:\n",
    "            items = [p.strip() for p in content.split(\"|\") if p.strip() and \"=\" not in p]\n",
    "        return \";\".join(items)\n",
    "\n",
    "    # repeat until nothing changes (nested templates)\n",
    "    while True:\n",
    "        new = pat_pipe.sub(repl_pipe, t)\n",
    "        new = pat_bul.sub(repl_bul, new)\n",
    "        if new == t:\n",
    "            break\n",
    "        t = new\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "\n",
    "def strip_wiki_markup(text: str) -> str:\n",
    "    t = html.unescape(text).replace(\"\\xa0\", \" \")\n",
    "    t = re.sub(r\"<!--.*?-->\", \" \", t, flags=re.DOTALL)\n",
    "\n",
    "    # refs\n",
    "    t = re.sub(r\"<ref[^>]*>.*?</ref>\", \" \", t, flags=re.IGNORECASE | re.DOTALL)\n",
    "    t = re.sub(r\"<ref[^/]*/>\", \" \", t, flags=re.IGNORECASE)\n",
    "\n",
    "    # drop cite/sfn templates\n",
    "    t = re.sub(r\"\\{\\{\\s*cite[^{}]*\\}\\}\", \" \", t, flags=re.IGNORECASE | re.DOTALL)\n",
    "    t = re.sub(r\"\\{\\{\\s*sfn[^{}]*\\}\\}\", \" \", t, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # unwrap lists (improved)\n",
    "    t = _unwrap_list_templates(t)\n",
    "\n",
    "    # external links\n",
    "    t = re.sub(r\"\\[https?://[^\\s\\]]+\\s+([^\\]]+)\\]\", r\"\\1\", t)  # [url label] -> label\n",
    "    t = re.sub(r\"https?://\\S+\", \" \", t)                        # bare urls -> drop\n",
    "\n",
    "    t = re.sub(r\"\\{\\{\\s*lang\\s*\\|\\s*[\\w-]+\\s*\\|\\s*([^{}|]+)(?:\\|[^{}]*)?\\}\\}\", r\"\\1\", t, flags=re.I)\n",
    "    t = re.sub(r\"\\{\\{\\s*nowrap\\s*\\|\\s*([^{}]+?)\\s*\\}\\}\", r\"\\1\", t, flags=re.I)\n",
    "\n",
    "    # html + wiki links\n",
    "    t = re.sub(r\"<[^>]+>\", \" \", t)\n",
    "    t = re.sub(r\"\\[\\[([^|\\]]+)\\|([^\\]]+)\\]\\]\", r\"\\2\", t)\n",
    "    t = re.sub(r\"\\[\\[([^\\]]+)\\]\\]\", r\"\\1\", t)\n",
    "\n",
    "    # line/item separators\n",
    "    t = t.replace(\"<br />\",\";\").replace(\"<br/>\",\";\").replace(\"<br>\",\";\")\n",
    "    t = t.replace(\"*\",\";\")\n",
    "    t = re.sub(r\"[·•]\", \";\", t)\n",
    "\n",
    "    # remove any remaining one-line templates\n",
    "    t = re.sub(r\"\\{\\{[^{}]*\\}\\}\", \" \", t, flags=re.DOTALL)\n",
    "\n",
    "    # rare leftover: naked 'hlist|' / 'flatlist|' beginnings\n",
    "    t = re.sub(r\"\\b(?:h\\s*list|flat\\s*list|plain\\s*list|ubl)\\s*\\|\", \" \", t, flags=re.IGNORECASE)\n",
    "\n",
    "    t = t.replace(\"\\n\", \";\")  # treat residual line breaks as item separators\n",
    "\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "CANON_MAP = {\n",
    "    \"pop music\": \"pop\",\n",
    "    \"adult contemporary music\": \"adult contemporary\",\n",
    "    \"rnb\": \"r&b\", \"r&b/soul\": \"r&b\",\n",
    "    \"hip hop music\": \"hip hop\",\n",
    "    \"electronic music\": \"electronic\",\n",
    "    \"dance music\": \"dance\",\n",
    "    \"rock & roll\": \"rock and roll\", \"rock 'n' roll\": \"rock and roll\",\n",
    "    \"rock'n'roll\": \"rock and roll\", \"rock n roll\": \"rock and roll\",\n",
    "    \"middle-of-the-road\": \"middle of the road\",\n",
    "    \"middle of the road (music)\": \"middle of the road\",\n",
    "}\n",
    "\n",
    "NON_GENRE_TRASH = re.compile(\n",
    "    r\"^\\s*(?:h\\s*list|flat\\s*list|plain\\s*list|ubl|artist)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "INSTRUMENT_WORDS = {\n",
    "    \"guitar\",\"vocals\",\"voice\",\"drums\",\"bass\",\"keyboards\",\"piano\",\"organ\",\"synthesizer\",\n",
    "    \"harmonica\",\"violin\",\"cello\",\"saxophone\",\"trumpet\",\"percussion\"\n",
    "}\n",
    "\n",
    "def postfilter_parts(parts: list[str]) -> list[str]:\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        q = p.strip()\n",
    "        if not q: \n",
    "            continue\n",
    "        if \"=\" in q:                   # key=value junk\n",
    "            continue\n",
    "        if NON_GENRE_TRASH.match(q):   # naked template names etc.\n",
    "            continue\n",
    "        if q.lower() in INSTRUMENT_WORDS:  # instruments (e.g., \"guitar\")\n",
    "            continue\n",
    "        out.append(q)\n",
    "    return out\n",
    "\n",
    "\n",
    "def split_genres(raw: str) -> list[str]:\n",
    "    s = raw\n",
    "    for sep in [\";\", \"•\", \"·\", \"—\", \"–\", \"/\", \",\"]:\n",
    "        s = s.replace(sep, \";\")\n",
    "    # optional nicety to split \"... rock ... rock\"\n",
    "    s = re.sub(r\"\\brock\\s+(?=[a-z]+\\s+rock\\b)\", \" ; \", s, flags=re.I)\n",
    "    return [p.strip().strip(\",;\") for p in s.split(\";\") if p.strip().strip(\",;\")]\n",
    "\n",
    "\n",
    "def normalize_genre(g: str) -> Optional[str]:\n",
    "    if not g: return None\n",
    "    g0 = g.strip().strip(\"{}\").lower()\n",
    "    # remove common disambiguators\n",
    "    g0 = re.sub(r\"\\s*\\((?:music|genre|band|musical group|style)\\)\\s*\", \"\", g0)\n",
    "    g0 = g0.replace(\"&\", \" and \")\n",
    "    g0 = re.sub(r\"\\bmusic\\b\", \"\", g0)\n",
    "    g0 = g0.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    g0 = re.sub(r\"\\s*-\\s*\", \"-\", g0)\n",
    "    g0 = CANON_MAP.get(g0, g0)\n",
    "    g0 = g0.replace(\"-\", \" \")\n",
    "    g0 = re.sub(r\"\\s+\", \" \", g0).strip(\" ;,\")\n",
    "    return g0 or None\n",
    "\n",
    "def normalize_list(genres: list[str]) -> list[str]:\n",
    "    seen, out = set(), []\n",
    "    for g in genres:\n",
    "        n = normalize_genre(g)\n",
    "        if n and n not in seen:\n",
    "            seen.add(n); out.append(n)\n",
    "    return out\n",
    "\n",
    "# ---------- Run ----------\n",
    "INPUT_FILE = \"../rock_artists.txt\"\n",
    "artists = [line.strip() for line in open(INPUT_FILE, encoding=\"utf-8\") if line.strip()]\n",
    "\n",
    "artist_to_genres = {}\n",
    "for i, raw in enumerate(artists, 1):\n",
    "    title, wtxt = fetch_wikitext_resilient(raw, sleep_between=0.25)\n",
    "    genres = []\n",
    "    if wtxt:\n",
    "        ib = extract_infobox(wtxt)\n",
    "        if ib:\n",
    "            raw_field = extract_field_from_infobox(ib, [\"genre\", \"genres\"])\n",
    "            if raw_field:\n",
    "                clean = strip_wiki_markup(raw_field)\n",
    "                parts = split_genres(clean)\n",
    "                parts = postfilter_parts(parts)  \n",
    "                genres = normalize_list(parts)\n",
    "    artist_to_genres[raw] = genres\n",
    "\n",
    "# --- Manual fallback for pages that are hard to scrape ---\n",
    "\n",
    "SKIP_TITLES = {\n",
    "    \"AllMusic\",             # not an artist/band\n",
    "}\n",
    "\n",
    "MANUAL_MAP = {\n",
    "    \"City and Colour\": [\n",
    "        \"folk\", \"indie folk\", \"acoustic\", \"alternative rock\", \"post hardcore\", \"screamo\", \"melodic hardcore\"\n",
    "    ],\n",
    "    \"Eagles (band)\": [\n",
    "        \"rock\", \"country rock\", \"soft rock\", \"folk rock\", \"pop rock\"\n",
    "    ],\n",
    "    \"Electric Light Orchestra\": [\n",
    "        \"art rock\", \"progressive rock\", \"symphonic rock\", \"pop rock\", \"glam rock\"\n",
    "    ],\n",
    "    \"Jack White\": [\n",
    "        \"blues rock\", \"garage rock revival\", \"alternative rock\", \"punk blues\", \"experimental rock\"\n",
    "    ],\n",
    "    \"R.E.M.\": [\n",
    "        \"alternative rock\", \"jangle pop\", \"college rock\", \"folk rock\", \"post punk\"\n",
    "    ],\n",
    "    \"The Monkees\": [\n",
    "        \"pop rock\", \"rock\", \"bubblegum\", \"psychedelia\"\n",
    "    ],\n",
    "    \"W.A.S.P. (band)\": [\n",
    "        \"heavy metal\", \"glam metal\", \"hard rock\", \"shock rock\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# 1) Drop non-artist pages (so they don't show up as \"empty\")\n",
    "for k in SKIP_TITLES:\n",
    "    artist_to_genres.pop(k, None)\n",
    "\n",
    "# 2) Fill empties from MANUAL_MAP (only if your scraper returned nothing)\n",
    "for artist, fallback_genres in MANUAL_MAP.items():\n",
    "    if not artist_to_genres.get(artist):  # empty or missing\n",
    "        artist_to_genres[artist] = fallback_genres\n",
    "\n",
    "\n",
    "with open(\"genres.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(artist_to_genres, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nSaved genres.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f3b88",
   "metadata": {},
   "source": [
    "### *Build alias index for network node matching*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9b8d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, json\n",
    "\n",
    "# --- simple normalizer used for index keys (short & stable) ---\n",
    "QUOTES = {\"'\":\"'\", \"'\":\"'\", \"\"\":'\"', \"\"\":'\"', \"´\":\"'\", \"`\":\"'\"}\n",
    "DASHES = {\"–\":\"-\", \"—\":\"-\", \"−\":\"-\"}\n",
    "\n",
    "def nn(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s).strip()\n",
    "    for a,b in QUOTES.items(): s = s.replace(a,b)\n",
    "    for a,b in DASHES.items(): s = s.replace(a,b)\n",
    "    s = s.replace(\"_\", \" \")\n",
    "    s = s.replace(\" & \", \" and \")\n",
    "    s = re.sub(r\"\\s*-\\s*\", \"-\", s)      # collapse spaces around hyphens\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.lower()\n",
    "\n",
    "def strip_the(s: str) -> str:\n",
    "    return re.sub(r\"^\\s*the\\s+\", \"\", s, flags=re.I)\n",
    "\n",
    "def drop_paren_suffix(s: str) -> str:\n",
    "    return re.sub(r\"\\s*\\([^)]*\\)\\s*$\", \"\", s).strip()\n",
    "\n",
    "def variants(name: str):\n",
    "    base = nn(name)\n",
    "    yield base\n",
    "    yield strip_the(base)\n",
    "    no_paren = drop_paren_suffix(base)\n",
    "    yield no_paren\n",
    "    yield strip_the(no_paren)\n",
    "\n",
    "# --- build alias index: normalized variant -> canonical JSON key (original title) ---\n",
    "alias_index = {}\n",
    "for title, gl in artist_to_genres.items():\n",
    "    # skip pages with empty genres\n",
    "    if not gl: \n",
    "        continue\n",
    "    for v in set(variants(title)):\n",
    "        alias_index[v] = title\n",
    "\n",
    "# --- hard aliases you discovered (graph label -> canonical title) ---\n",
    "# (we store them in the same index by normalizing the LEFT side)\n",
    "hard_aliases = {\n",
    "    \"Bachman-Turner_Overdrive\": \"Bachman –Turner Overdrive\",\n",
    "    \"The_Go-Go's\":              \"The Go -Go's\",\n",
    "    \"Parliament-Funkadelic\":    \"Parliament -Funkadelic\",\n",
    "    \"The_All-American_Rejects\": \"The All -American Rejects\",\n",
    "    \"Dallas_Green_(musician)\":  \"City and Colour\",\n",
    "    \"Dallas Green\":             \"City and Colour\",\n",
    "    \"AllMusic\":                 None,                         # skip\n",
    "}\n",
    "for left, right in hard_aliases.items():\n",
    "    alias_index[nn(left)] = right\n",
    "\n",
    "# --- save the alias index for name matching ---\n",
    "with open(\"alias_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(alias_index, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c41c8",
   "metadata": {},
   "source": [
    "### *Create subgraph with genres*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb06c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved processed network:\n",
      "   • rock_network_with_genres.gexf: 485 nodes, 8,019 edges\n",
      "   • resolved_mapping.json: 485 node mappings\n"
     ]
    }
   ],
   "source": [
    "# --- Create and save processed network subgraph ---\n",
    "import networkx as nx\n",
    "\n",
    "# Load the original network\n",
    "G = nx.read_gexf(\"../graph/rock_network.gexf\")\n",
    "\n",
    "# Create resolved mapping (network node -> canonical artist name)\n",
    "resolved = {}\n",
    "for n in G.nodes():\n",
    "    canonical = alias_index.get(nn(n))\n",
    "    if canonical and canonical in artist_to_genres and artist_to_genres[canonical]:\n",
    "        resolved[n] = canonical\n",
    "\n",
    "# Create filtered subgraph with only matched nodes that have genres\n",
    "H = G.subgraph(list(resolved.keys())).copy()\n",
    "\n",
    "# Save the processed subgraph and resolved mapping\n",
    "nx.write_gexf(H, \"rock_network_with_genres.gexf\")\n",
    "with open(\"resolved_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(resolved, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Saved processed network:\")\n",
    "print(f\"rock_network_with_genres.gexf: {H.number_of_nodes():,} nodes, {H.number_of_edges():,} edges\")\n",
    "print(f\"resolved_mapping.json: {len(resolved):,} node mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6faf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
